{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ì‹¤ìŠµ_BERT_í† í¬ë‚˜ì´ì €_ì‚¬ìš©í•˜ê¸°_ìˆ˜ì •.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/idjoopal/NLP_tensorflow2.0/blob/main/%EC%8B%A4%EC%8A%B5_BERT_%ED%86%A0%ED%81%AC%EB%82%98%EC%9D%B4%EC%A0%80_%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0_%EC%88%98%EC%A0%95.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyLlLw0muJK4"
      },
      "source": [
        "# BERT í† í¬ë‚˜ì´ì € ì‹¤ìŠµ\n",
        "\n",
        "ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” êµ¬ê¸€ì—ì„œ ê³µê°œí•œ Multi-lingual BERTë¥¼ ë‹¤ìš´ë¡œë“œí•´ ì‚¬ìš©í•´ë³´ê² ìŠµë‹ˆë‹¤.   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFKvHZ6zmql8"
      },
      "source": [
        "## #1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ë¡œë”©"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg7SJ1s6mxvA"
      },
      "source": [
        "#### â–¶ pipë¡œ bert-for-tf2 ì„¤ì¹˜í•˜ê¸°\n",
        "\n",
        "bert-for-tf2 íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ë©´ BERT tokenizerì„ ì•„ì£¼ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qPNJqqo6BWQ"
      },
      "source": [
        "ì •ìƒì ì¸Â konlpyÂ ë¡œë”©ì„Â ìœ„í•´Â ì•„ë˜Â ì½”ë“œë¥¼Â ì‹¤í–‰í•œÂ í›„Â ëŸ°íƒ€ì„Â ë‹¤ì‹œÂ ì‹œì‘ì„Â ëˆŒëŸ¬ì£¼ì„¸ìš”!Â "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLUI6zuUdTyz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b080feb5-02b7-4bf6-fa3e-ae1f3dadacf5"
      },
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install konlpy\n",
        "\n",
        "!pipÂ installÂ jpype1==0.7.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/d3/820ccaf55f1e24b5dd43583ac0da6d86c2d27bbdfffadbba69bafe73ca93/bert-for-tf2-0.14.7.tar.gz (41kB)\n",
            "\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 10kB 23.4MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 20kB 20.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰        | 30kB 15.9MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 40kB 14.5MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 6.3MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.7-cp36-none-any.whl size=30539 sha256=e76580dabf887d9eacbf741265b7b22cfc10fd99119145edebe5838ad8b9197a\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/f8/e2/b98f79a6b8cc898d8e4102b83acb8a098df7d27500a2bac912\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7304 sha256=7139dfe0ff0387eb4d90540a429bc3d620b61ab10e255944741127e675cff713\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-cp36-none-any.whl size=19475 sha256=f141d6c42a8eb5a68ffb1ab9fd7f595d95da1c92af911fe60a524085f7caa8e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.7 params-flow-0.8.2 py-params-0.9.7\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19.4MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92kB 14.0MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/96/1030895dea70855a2e1078e3fe0d6a63dcb7c212309e07dc9ee39d33af54/JPype1-1.1.2-cp36-cp36m-manylinux2010_x86_64.whl (450kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 460kB 50.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: colorama, tweepy, beautifulsoup4, JPype1, konlpy\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.1.2 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.9.0\n",
            "/bin/bash: pipÂ installÂ jpype1==0.7.0: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfZQqipinJc-"
      },
      "source": [
        "#### â–¶ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”©\n",
        "ë°©ê¸ˆ ì„¤ì¹˜í•œ bert íŒ¨í‚¤ì§€ì™€ TensorFlow Hubë¥¼ ë¡œë”©í•˜ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISF7wYaEnVdW"
      },
      "source": [
        "## bert ëª¨ë“ˆ ë¡œë”© & TF hub ì—°ê²°\n",
        "\n",
        "import bert\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6TuaxlQwiob"
      },
      "source": [
        "## #2. ì‚¬ì „í•™ìŠµëœ BERT ëª¨ë¸ ë¡œë”©\n",
        "Tensorflow hubì—ì„œ pretrainëœ ë‹¤êµ­ì–´ BERT ëª¨ë¸ì„ ê°€ì ¸ì˜¤ëŠ” ì½”ë“œì…ë‹ˆë‹¤.   \n",
        "í™ˆí˜ì´ì§€ì—ì„œ Multi-lingula BERTì— í•´ë‹¹í•˜ëŠ” ì£¼ì†Œë¥¼ ë³µì‚¬í•´ BERT_MODEL_HUBì— ì…ë ¥í–ˆìŠµë‹ˆë‹¤. \n",
        "\n",
        "ê·¸ë¦¬ê³  hub.KerasLayer í•¨ìˆ˜ë¥¼ ì´ìš©í•´ bert_layerë¥¼ ê°€ì§€ê³  ì™”ìŠµë‹ˆë‹¤.   \n",
        "ì´ ë ˆì´ì–´ê°€ ë°”ë¡œ Transformer ì¸ì½”ë”ê°€ 12ì¸µ ìŒ“ì—¬ìˆëŠ” BERT ëª¨ë¸ì…ë‹ˆë‹¤.   \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvcbF_WliIPs"
      },
      "source": [
        "BERT_MODEL_HUB = 'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2'\n",
        "\n",
        "# BERT layer ê°€ì ¸ì˜¤ê¸°\n",
        "bert_layer = hub.KerasLayer(BERT_MODEL_HUB, trainable=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hktrgEmu3pz"
      },
      "source": [
        "## #3. BERT parsing ì´í•´í•˜ê¸°\n",
        "- BERTì—ì„œëŠ” Wordpiece Tokenizationì„ í†µí•´ í† í°ì„ subtokenìœ¼ë¡œ ìª¼ê°­ë‹ˆë‹¤.    \n",
        "- í•œêµ­ì–´ì˜ ê²½ìš° ì›í˜•ì„ ë³´ì¡´í•˜ëŠ” í˜•íƒœì†Œ ë¶„ì„ì„ ê±°ì¹œ í›„ subtokenìœ¼ë¡œ ìª¼ê°œëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.   \n",
        "- ìœ„ì—ì„œ ë¡œë”©í•œ bert_layerì—ì„œ ì‚¬ì „í•™ìŠµì— í™œìš©í•œ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë”©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
        "   - bert.tokenization.bert_tokenization í•¨ìˆ˜ ì‚¬ìš©\n",
        "   - í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì´ìš©í•´ ë¬¸ì¥ì„ í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ìª¼ê°  í›„\n",
        "   - tokenizerì˜ <font color=\"blue\">FullTokenizer</font> ì„ ì‚¬ìš©í•´ Sub-tokenization ì§„í–‰\n",
        "\n",
        "- ìš°ë¦¬ê°€ Pythonìœ¼ë¡œ ì½”ë”©í–ˆë˜ @convert_tokens_to_idsë‚˜ @convert_ids_to_tokensë§¤ì„œë“œê°€ bert íŒ¨í‚¤ì§€ì— ëª¨ë‘ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSu5oGnxwqgd"
      },
      "source": [
        "#### Step 1. í† í¬ë‚˜ì´ì € ë¡œë”©í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsSbSQ10uj7m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f61bb42-d6ff-4c6d-cf9f-4b2825609dad"
      },
      "source": [
        "from  bert.tokenization import bert_tokenization\n",
        "\n",
        "# vocab_file ê°€ì ¸ì˜¤ê¸°\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "\n",
        "# ì†Œë¬¸ìí™”ë¥¼ í•˜ëŠ”ì§€ ì—¬ë¶€ ê°€ì ¸ì˜¤ê¸°\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "\n",
        "# í† í¬ë‚˜ì´ì € ë¡œë”©\n",
        "print(\"vocab file:\", vocab_file)\n",
        "print(\"do_lower_case:\", do_lower_case)\n",
        "\n",
        "tokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab file: b'gs://tfhub-modules/tensorflow/bert_multi_cased_L-12_H-768_A-12/2/uncompressed/assets/vocab.txt'\n",
            "do_lower_case: False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asdN3KjDrNZE"
      },
      "source": [
        "ğŸ‘‰ BERTì—ì„œ ì‚¬ìš©í•˜ëŠ” ë‹¨ì–´ì‚¬ì „ì´ ìœ„ì— í”„ë¦°íŠ¸ëœ ê²½ë¡œì— txt íŒŒì¼ë¡œ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.   \n",
        "ğŸ‘‰ ì†Œë¬¸ìí™”ë¥¼ ì§„í–‰í•˜ì—¬ í•™ìŠµí•œ BERTë„ ìˆì§€ë§Œ ë‹¤êµ­ì–´ ëª¨ë¸ì€ ì†Œë¬¸ìí™”ë¥¼ í•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— do_lower_case=Falseì¸ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42k2GhFMvwan",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c567e178-7d70-4b9f-ddbd-84a803a7fd61"
      },
      "source": [
        "# vocab ì‚¬ì „ í™•ì¸í•˜ê¸°\n",
        "\n",
        "print(\"ë‹¨ì–´ì‚¬ì „ì— ìˆëŠ” í† í° ê°œìˆ˜:\", len(tokenizer.vocab))\n",
        "print(\"ì˜ˆì‹œ:\", list(tokenizer.vocab.keys())[:300])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ë‹¨ì–´ì‚¬ì „ì— ìˆëŠ” í† í° ê°œìˆ˜: 119547\n",
            "ì˜ˆì‹œ: ['[PAD]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]', '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]', '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]', '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]', '[unused49]', '[unused50]', '[unused51]', '[unused52]', '[unused53]', '[unused54]', '[unused55]', '[unused56]', '[unused57]', '[unused58]', '[unused59]', '[unused60]', '[unused61]', '[unused62]', '[unused63]', '[unused64]', '[unused65]', '[unused66]', '[unused67]', '[unused68]', '[unused69]', '[unused70]', '[unused71]', '[unused72]', '[unused73]', '[unused74]', '[unused75]', '[unused76]', '[unused77]', '[unused78]', '[unused79]', '[unused80]', '[unused81]', '[unused82]', '[unused83]', '[unused84]', '[unused85]', '[unused86]', '[unused87]', '[unused88]', '[unused89]', '[unused90]', '[unused91]', '[unused92]', '[unused93]', '[unused94]', '[unused95]', '[unused96]', '[unused97]', '[unused98]', '[unused99]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '<S>', '<T>', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', 'Â¡', 'Â¢', 'Â£', 'Â¥', 'Â¦', 'Â§', 'Â¨', 'Â©', 'Âª', 'Â«', 'Â¬', 'Â®', 'Â°', 'Â±', 'Â²', 'Â³', 'Âµ', 'Â¶', 'Â·', 'Â¹', 'Âº', 'Â»', 'Â¼', 'Â½', 'Â¾', 'Â¿', 'Ã€', 'Ã', 'Ã‚', 'Ãƒ', 'Ã„', 'Ã…', 'Ã†', 'Ã‡', 'Ãˆ', 'Ã‰', 'ÃŠ', 'Ã‹', 'ÃŒ', 'Ã', 'Ã', 'Ã', 'Ã‘', 'Ã’', 'Ã“', 'Ã”', 'Ã•', 'Ã–', 'Ã—', 'Ã˜', 'Ãš', 'Ãœ', 'Ã', 'Ã', 'ÃŸ', 'Ã ', 'Ã¡', 'Ã¢', 'Ã£', 'Ã¤', 'Ã¥', 'Ã¦', 'Ã§', 'Ã¨', 'Ã©', 'Ãª', 'Ã«', 'Ã¬', 'Ã­', 'Ã®', 'Ã¯', 'Ã°', 'Ã±', 'Ã²', 'Ã³', 'Ã´', 'Ãµ', 'Ã¶', 'Ã·', 'Ã¸', 'Ã¹', 'Ãº', 'Ã»', 'Ã¼', 'Ã½', 'Ã¾', 'Ã¿', 'Ä€', 'Ä', 'Ä‚', 'Äƒ', 'Ä„', 'Ä…', 'Ä†', 'Ä‡', 'ÄŒ', 'Ä', 'Ä', 'Ä', 'Ä', 'Ä‘']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Sp8PwK7rpJR"
      },
      "source": [
        "ğŸ‘‰ ì´ 119,547ê°œì˜ í† í°ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.   \n",
        "ğŸ‘‰ [PAD] í† í°ë¶€í„° ì‹œì‘í•´ unusedë¡œ ì˜ˆì•½ëœ ìë¦¬ê°€ ìˆê³ , ì˜ì–´, ëŸ¬ì‹œì•„ì–´(?) ë“± ë‹¤ì–‘í•œ ì–¸ì–´ì˜ í† í°ë“¤ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.   \n",
        "ğŸ‘‰ í•œêµ­ì–´ë§Œì„ ìœ„í•œ ëª¨ë¸ì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì—, í•œêµ­ì–´ë§Œìœ¼ë¡œ ì‚¬ì „í•™ìŠµí•œ BERTì— ë¹„í•´ì„œëŠ” ì„±ëŠ¥ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤ ã… .ã… "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCkpLouDmAJr"
      },
      "source": [
        "\"\"\" í˜•íƒœì†Œ ë¶„ì„ í•¨ìˆ˜ \"\"\"\n",
        "from konlpy.tag import Okt\n",
        "okt=Okt()\n",
        "\n",
        "def tokenize(lines):\n",
        "  return okt.morphs(lines)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o089Zp2-Ijp"
      },
      "source": [
        "ğŸ‘‰ 1~2ì¼ì°¨ ì‹¤ìŠµì—ì„œ ì €í¬ëŠ” Komoran í˜•íƒœì†Œë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.    \n",
        "í•˜ì§€ë§Œ Komoranì€ ì›í˜•ì„ ë³µì›í•˜ì—¬ í˜•íƒœì†Œë¥¼ ë¶„ì„í•˜ëŠ” ì•„ì´ì˜€ìŠµë‹ˆë‹¤.   \n",
        "\n",
        "ğŸ‘‰ Sub-tokenizingì„ ìœ„í•´ì„œëŠ” ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ìª¼ê°œê¸°ë§Œ í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì—, okt ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxT95BwawwXU"
      },
      "source": [
        "#### Step 2. í˜•íƒœì†Œ ë¶„ì„ + Subtokenization ì‹¤í–‰í•˜ê¸°\n",
        "- tokenizer.tokenize ì‚¬ìš©"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i20ZPyEMQJYl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d870a19e-3060-405b-8f74-b45431d8b821"
      },
      "source": [
        "sentence = \"ë²„íŠ¸ë¡œ í† í¬ë‚˜ì´ì¦ˆí•˜ëŠ” ì˜ˆì œ\"\n",
        "\n",
        "# basic_tokenizerë¡œ ë¬¸ì¥ ìª¼ê°œê¸°\n",
        "tokenized_sentence = tokenize(sentence)\n",
        "print(tokenized_sentence)\n",
        "\n",
        "# BPEë¡œ ë¬¸ì¥ ìª¼ê°œê¸°\n",
        "sub_tokens = tokenizer.tokenize(\" \".join(tokenized_sentence))\n",
        "print(sub_tokens)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ë²„íŠ¸', 'ë¡œ', 'í† í¬', 'ë‚˜', 'ì´ì¦ˆ', 'í•˜', 'ëŠ”', 'ì˜ˆì œ']\n",
            "['ë²„', '##íŠ¸', 'ë¡œ', 'í† ', '##í¬', 'ë‚˜', 'ì´', '##ì¦ˆ', 'í•˜', 'ëŠ”', 'ì˜ˆ', '##ì œ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OZPX2p9uzjd"
      },
      "source": [
        "ğŸ‘‰ tokenizer.tokenize í•¨ìˆ˜ë¥¼ í†µí•´ í˜•íƒœì†Œ ë¶„ì„ëœ ë¬¸ì¥ì„ WordPiece ë‹¨ìœ„ë¡œ ìª¼ê°­ë‹ˆë‹¤. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHWQn07B4Oq5"
      },
      "source": [
        "ğŸ™†â€â™€ï¸ ì›í•˜ëŠ” ìì—°ì–´ ë¬¸ì¥ì„ BERT tokenizerë¡œ ìª¼ê°œê³  ê²°ê³¼ë¥¼ í™•ì¸í•´ ë³´ì„¸ìš”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ghjyqZN4M8g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed75b015-6835-46cc-cc81-efc133ed62c0"
      },
      "source": [
        "sentence = \"í–„ë²„ê±°ëŠ” ì—­ì‹œ ë²„ê±°í‚¹\"\n",
        "\n",
        "# basic_tokenizerë¡œ ë¬¸ì¥ ìª¼ê°œê¸°\n",
        "tokenized_sentence = tokenize(sentence)\n",
        "print(tokenized_sentence)\n",
        "\n",
        "# Sub-tokenìœ¼ë¡œ ìª¼ê°œê¸°\n",
        "print(tokenizer.tokenize(\" \".join(tokenized_sentence)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['í–„ë²„ê±°', 'ëŠ”', 'ì—­ì‹œ', 'ë²„ê±°í‚¹']\n",
            "['í–„', '##ë²„', '##ê±°', 'ëŠ”', 'ì—­ì‹œ', 'ë²„', '##ê±°', '##í‚¹']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPOwbFPOwzIU"
      },
      "source": [
        "#### Step 3. BPE í† í°ì„ ëª¨ë¸ ì¸í’‹ ì¸ë±ìŠ¤ë¡œ ë°”ê¾¸ê¸°\n",
        "- tokenizer.convert_tokens_to_idsë¥¼ ì‚¬ìš©í•˜ë©´ Subtokenì„ ì¸ë±ìŠ¤ë¡œ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
        "- ìš°ë¦¬ê°€ ì½”ë”©í•´ì„œ ì‚¬ìš©í–ˆë˜ ë°©ì‹ê³¼ ë™ì¼í•˜ê²Œ ì‘ë™í•©ë‹ˆë‹¤. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGhw8GKGQJUe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dfe7f4f-ca31-4ca2-8b39-a4746fe78e07"
      },
      "source": [
        "# ëª¨ë¸ ì¸í’‹ ì¸ë±ìŠ¤ë¡œ ë°”ê¾¸ê¸°\n",
        "print(sub_tokens)\n",
        "input_ids = tokenizer.convert_tokens_to_ids(sub_tokens)\n",
        "print(input_ids)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ë²„', '##íŠ¸', 'ë¡œ', 'í† ', '##í¬', 'ë‚˜', 'ì´', '##ì¦ˆ', 'í•˜', 'ëŠ”', 'ì˜ˆ', '##ì œ']\n",
            "[9336, 15184, 9202, 9873, 20308, 8982, 9638, 24891, 9952, 9043, 9576, 17730]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQFbO9yy4MAz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0a4e1c6-4b62-4d38-dd20-02b62bce1957"
      },
      "source": [
        "# ì¸í’‹ ì¸ë±ìŠ¤ë¥¼ í† í°ìœ¼ë¡œ ë°”ê¾¸ê¸°\n",
        "reversed_token = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "print(reversed_token)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ë²„', '##íŠ¸', 'ë¡œ', 'í† ', '##í¬', 'ë‚˜', 'ì´', '##ì¦ˆ', 'í•˜', 'ëŠ”', 'ì˜ˆ', '##ì œ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwdOTe6ovOgE"
      },
      "source": [
        "## #4. BERT vocab ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆí•˜ê¸°\n",
        "BERTì—ëŠ” ë¬´ë ¤ 99ê°œì˜ unused í† í° ìë¦¬ê°€ ì˜ˆì•½ë˜ì–´ ìˆìŠµë‹ˆë‹¤.   \n",
        "ì´ ìë¦¬ë¥¼ ì–´ë–¤ ì‹ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆì„ê¹Œìš”?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t63rLXQYI1bA"
      },
      "source": [
        "<font color = \"blue\">\n",
        "[ìˆ˜ì •] Bert íŒ¨í‚¤ì§€ì—ì„œ ì›ë˜ vocab íŒŒì¼ì„ ë¡œì»¬ë¡œ ë‹¤ìš´ë¡œë“œí•´ì£¼ì—ˆëŠ”ë°,    \n",
        "ì§€ê¸ˆì€ google storageì—ì„œ ë°”ë¡œ ì—°ë™í•˜ì—¬ ì‚¬ìš©í•˜ë„ë¡ íŒ¨í‚¤ì§€ ì—…ë°ì´íŠ¸ê°€ ëœ ê²ƒ ê°™ìŠµë‹ˆë‹¤. \n",
        "\n",
        "Google Storageì—ì„œ vocab.txt íŒŒì¼ì„ Colab ë¡œì»¬ë¡œ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ì½”ë“œë¥¼ ì¶”ê°€í•˜ê² ìŠµë‹ˆë‹¤!</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXtoq1M8JE39",
        "outputId": "ffac38b4-4027-4e6c-8242-b8b124189ad0"
      },
      "source": [
        "!gsutil cp gs://tfhub-modules/tensorflow/bert_multi_cased_L-12_H-768_A-12/2/uncompressed/assets/vocab.txt /content/vocab.txt \n",
        "vocab_file = \"/content/vocab.txt\""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://tfhub-modules/tensorflow/bert_multi_cased_L-12_H-768_A-12/2/uncompressed/assets/vocab.txt...\n",
            "/ [0 files][    0.0 B/972.2 KiB]                                                \r/ [1 files][972.2 KiB/972.2 KiB]                                                \r\n",
            "Operation completed over 1 objects/972.2 KiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTr0ZzG-vcoy"
      },
      "source": [
        "ë¨¼ì € ì›ë˜ ë‹¨ì–´ì‚¬ì „ text íŒŒì¼ì„ ì—´ì–´ org_vocabsë¼ëŠ” ë¦¬ìŠ¤íŠ¸ì— ì½ì–´ì˜¤ê² ìŠµë‹ˆë‹¤. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRf8Y8qiGlUX"
      },
      "source": [
        "## ì›ë˜ ë‹¨ì–´ ì‚¬ì „ í™•ì¸í•˜ê¸°\n",
        "\n",
        "with open(vocab_file) as f:\n",
        "  org_vocabs = [s.strip() for s in f.readlines()]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJpoq_a3GlOp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "744c493b-dec8-4f39-b885-f90850ea430a"
      },
      "source": [
        "print(\"# vocabs:\", len(org_vocabs))\n",
        "print(org_vocabs[:101])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# vocabs: 119547\n",
            "['[PAD]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]', '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]', '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]', '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]', '[unused49]', '[unused50]', '[unused51]', '[unused52]', '[unused53]', '[unused54]', '[unused55]', '[unused56]', '[unused57]', '[unused58]', '[unused59]', '[unused60]', '[unused61]', '[unused62]', '[unused63]', '[unused64]', '[unused65]', '[unused66]', '[unused67]', '[unused68]', '[unused69]', '[unused70]', '[unused71]', '[unused72]', '[unused73]', '[unused74]', '[unused75]', '[unused76]', '[unused77]', '[unused78]', '[unused79]', '[unused80]', '[unused81]', '[unused82]', '[unused83]', '[unused84]', '[unused85]', '[unused86]', '[unused87]', '[unused88]', '[unused89]', '[unused90]', '[unused91]', '[unused92]', '[unused93]', '[unused94]', '[unused95]', '[unused96]', '[unused97]', '[unused98]', '[unused99]', '[UNK]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmRe37uGOxKX"
      },
      "source": [
        "ì´ë²ˆ í”„ë¡œì íŠ¸ë¡œ LG CNS ë¸”ë¡œê·¸ ëŒ“ê¸€ì— ëŒ€í•œ ê°ì„± ëª¨ë‹ˆí„°ë§ ê³¼ì œë¥¼ ìˆ˜í–‰í•˜ë ¤ê³  í•©ë‹ˆë‹¤.   \n",
        "ìì‚¬ì˜ ë¸”ë¡œê·¸ì´ë‹¤ë³´ë‹ˆ \\<CNS>, <ì—˜ì§€> ê°™ì€ ë‹¨ì–´ë“¤ì´ ë§ì´ ë³´ì…ë‹ˆë‹¤.   \n",
        "\n",
        "ì €í¬ íšŒì‚¬ ì´ë¦„ì´ ë“¤ì–´ê°„ ë§Œí¼ ì´ í† í°ë“¤ì€ subwordë¡œ í† í¬ë‚˜ì´ì¦ˆë˜ëŠ” ëŒ€ì‹  í•˜ë‚˜ì˜ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„ì„í•˜ê³  ì‹¶ì€ë°ìš”,   \n",
        "ë¨¼ì € ì›ë˜ BERT ë‹¨ì–´ì‚¬ì „ì— ì´ ë‹¨ì–´ë“¤ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5g5Gu7YGlLZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a549395-bf32-4999-c601-cf12da72af3b"
      },
      "source": [
        "print(\"CNS\" in org_vocabs)\n",
        "print(\"ì—˜ì§€\" in org_vocabs)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A7hHqZawGUd"
      },
      "source": [
        "ğŸ‘‰ ì´ëŸ°, êµ¬ê¸€ì´ ê³µê°œí•œ BERTì˜ ë‹¨ì–´ì‚¬ì „ì—ëŠ” ì´ í† í°ë“¤ì´ í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.   \n",
        "ğŸ‘‰ ê·¸ë ‡ë‹¤ë©´ ì§€ê¸ˆì€ ì´ëŸ° í† í°ë“¤ì´ í¬í•¨ëœ ë¬¸ì¥ì€ ì–´ë–»ê²Œ íŒŒì‹±ë˜ê³  ìˆëŠ”ì§€ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGRwcYo5IIU8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbe597b7-197b-48a1-ebff-b71c1c67dc9d"
      },
      "source": [
        "tokenized = tokenizer.tokenize(\"ì•ˆë…•í•˜ì„¸ìš” ì—˜ì§€ CNS ì„ìŠ¹ì˜ ì„ ì„ ì—°êµ¬ì›ì…ë‹ˆë‹¤.\")\n",
        "input_ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
        "print(input_ids)\n",
        "reversed_token = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "print(reversed_token)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9521, 118741, 35506, 24982, 48549, 9562, 12508, 73067, 10731, 9644, 48210, 30858, 9428, 36240, 91785, 14279, 58303, 48345, 119]\n",
            "['ì•ˆ', '##ë…•', '##í•˜', '##ì„¸', '##ìš”', 'ì—˜', '##ì§€', 'CN', '##S', 'ì„', '##ìŠ¹', '##ì˜', 'ì„ ', '##ì„', 'ì—°êµ¬', '##ì›', '##ì…', '##ë‹ˆë‹¤', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbQe1R5bwX-T"
      },
      "source": [
        "ğŸ‘‰ Vocabì— ë‹¨ì–´ê°€ ì—†ë‹¤ë³´ë‹ˆ CNSëŠ” CN ##S , ì—˜ì§€ëŠ” ì—˜ ##ì§€ ë¡œ ì°¢ì–´ì ¸ì„œ í† í¬ë‚˜ì´ì§•ë˜ê³  ìˆìŠµë‹ˆë‹¤. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fP7sRx1wx2Y"
      },
      "source": [
        "ì´ëŸ° í˜„ìƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´, ë¶„ë¦¬ë˜ì§€ ì•Šê³  ë¶„ì„ë˜ì—ˆìœ¼ë©´ í•˜ëŠ” í† í°ë“¤ì„ ì¶”ê°€í•´ ìƒˆë¡œìš´ ë‹¨ì–´ì‚¬ì „ì„ ë§Œë“¤ê³    \n",
        "ì´ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥í•˜ê² ìŠµë‹ˆë‹¤. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxRXN9KpJHgt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc307a72-9cee-4db7-f68e-503d90cd8aab"
      },
      "source": [
        "never_split = [\"ì—˜ì§€\", \"CNS\"]\n",
        "\n",
        "## ì¶”ê°€í•œ never_split ë‹¨ì–´ë¥¼ ë°˜ì˜í•´ ìƒˆë¡œìš´ ì‚¬ì „ì„ ë§Œë“¤ì–´ì£¼ê¸°\n",
        "new_vocabs = org_vocabs.copy()\n",
        "idx = 1\n",
        "for tok in never_split:\n",
        "  if tok not in org_vocabs: # (ì•ˆì „ì¥ì¹˜ 1) ì›ë˜ vocabì— ì—†ìœ¼ë©´\n",
        "    if \"unused\" in new_vocabs[idx]: # (ì•ˆì „ì¥ì¹˜ 2) [unused] í† í° ìë¦¬ì´ë©´\n",
        "      new_vocabs[idx] = tok\n",
        "      print(\"{} -> {}\".format(org_vocabs[idx], new_vocabs[idx]))\n",
        "      idx += 1\n",
        "    else:\n",
        "      \"Cannot Allocate New Token Anymore\"\n",
        "      break"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[unused1] -> ì—˜ì§€\n",
            "[unused2] -> CNS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nbDGdKaw9-I"
      },
      "source": [
        "ğŸ‘‰ new_vocabsì—ëŠ” never_splitìœ¼ë¡œ ì •í•œ í† í°ë“¤ì„ í¬í•¨í•œ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ê°€ ì €ì¥ë©ë‹ˆë‹¤. \n",
        "\n",
        "ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ì¶”ê°€í•˜ëŠ” ê³¼ì •ì—ì„œëŠ” ë‘ ê°€ì§€ ì•ˆì „ì¥ì¹˜ë¥¼ ë„£ì–´ì£¼ì—ˆìŠµë‹ˆë‹¤. \n",
        "1. ì›ë˜ vocabì— ì—†ëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€í•˜ê¸° -> ë‹¨ì–´ì‚¬ì „ì—ëŠ” ì¤‘ë³µì´ ìˆìœ¼ë©´ ì•ˆ ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.   \n",
        "2. [unused #] í† í°ì¼ ë•Œë§Œ ëŒ€ì²´í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqGmm2aGqMyj"
      },
      "source": [
        "## ìƒˆ ë‹¨ì–´ì‚¬ì „ ì €ì¥\n",
        "new_vocab_file = \"/content/new_vocab.txt\"\n",
        "\n",
        "with open(new_vocab_file, \"w\") as f:\n",
        "  f.write(\"\\n\".join(new_vocabs))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-4vcsdOxz6T"
      },
      "source": [
        "ì´ì œ ìƒˆë¡œ ì €ì¥í•œ ë‹¨ì–´ì‚¬ì „ì„ ì‚¬ìš©í•´ new_tokenizerë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ë‹¤ì‹œ í•œ ë²ˆ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë”©í•˜ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeYyFCbcqSm0"
      },
      "source": [
        "## ìƒˆë¡œìš´ ì‚¬ì „ì„ ì´ìš©í•´ ë¡œë”©\n",
        "\n",
        "new_tokenizer = bert_tokenization.FullTokenizer(new_vocab_file, do_lower_case)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDvr0xwVJQs5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4994a851-c828-4933-8c79-e768e93f7df5"
      },
      "source": [
        "tokenized = new_tokenizer.tokenize(\"ì•ˆë…•í•˜ì„¸ìš” ì—˜ì§€ CNS ì„ìŠ¹ì˜ ì„ ì„ ì—°êµ¬ì›ì…ë‹ˆë‹¤.\")\n",
        "print(tokenized)\n",
        "input_ids = new_tokenizer.convert_tokens_to_ids(tokenized)\n",
        "print(input_ids)\n",
        "reversed_token = new_tokenizer.convert_ids_to_tokens(input_ids)\n",
        "print(reversed_token)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ì•ˆ', '##ë…•', '##í•˜', '##ì„¸', '##ìš”', 'ì—˜ì§€', 'CNS', 'ì„', '##ìŠ¹', '##ì˜', 'ì„ ', '##ì„', 'ì—°êµ¬', '##ì›', '##ì…', '##ë‹ˆë‹¤', '.']\n",
            "[9521, 118741, 35506, 24982, 48549, 1, 2, 9644, 48210, 30858, 9428, 36240, 91785, 14279, 58303, 48345, 119]\n",
            "['ì•ˆ', '##ë…•', '##í•˜', '##ì„¸', '##ìš”', 'ì—˜ì§€', 'CNS', 'ì„', '##ìŠ¹', '##ì˜', 'ì„ ', '##ì„', 'ì—°êµ¬', '##ì›', '##ì…', '##ë‹ˆë‹¤', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYEN2AfLyWMA"
      },
      "source": [
        "ğŸ‘‰ í† í°ì„ ì¶”ê°€í–ˆê¸° ë•Œë¬¸ì— ì´ë²ˆì—ëŠ” \"ì—˜ì§€\"ì™€ \"CNS\"ê°€ ìª¼ê°œì§€ì§€ ì•Šê³  í† í¬ë‚˜ì´ì§•ë˜ì—ˆìŠµë‹ˆë‹¤.   \n",
        "ğŸ‘‰ ë¸”ë¡œê·¸ ëŒ“ê¸€ì— ëŒ€í•œ í•™ìŠµ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ëŠ” ê³¼ì •ì—ì„œ BERTëŠ” fine-tuningì„ í†µí•´ ìƒˆë¡œ ì¶”ê°€ëœ í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ë¥¼ í•™ìŠµí•˜ê²Œ ë  ê²ƒì…ë‹ˆë‹¤. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKyRlIK8ynsQ"
      },
      "source": [
        "## #5. DAILY MISSION   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt693N4gzFSL"
      },
      "source": [
        "<font color=\"red\">MISSION: BERT í† í¬ë‚˜ì´ì§• & ì¸ë±ì‹± í•´ë³´ê¸° </font>\n",
        "\n",
        "BERTë¥¼ ì‚¬ìš©í•´ ê°ì„±ë¶„ì„ ê³¼ì œë¥¼ ìˆ˜í–‰í•˜ê³ ì í•©ë‹ˆë‹¤.    \n",
        "ê°ì„±ë¶„ì„ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” ì¸í’‹ ë¬¸ì¥ì„ <b>[CLS] ì¸í’‹ë¬¸ì¥ [SEP]</b>ì˜ í˜•íƒœë¡œ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤. \n",
        "\n",
        "ì¸í’‹ ë¬¸ì¥ìœ¼ë¡œ \"BERT ì•Œê³  ë³´ë‹ˆ ì™„ì „ ì‰½ë„¤\"ë¼ëŠ” ë¬¸ì¥ì´ ë“¤ì–´ì™”ìŠµë‹ˆë‹¤.   \n",
        "\n",
        "1. í˜•íƒœì†Œë¶„ì„ê³¼ BERT í† í¬ë‚˜ì´ì§•ì„ ì§„í–‰í•´ ìœ„ì˜ ë¬¸ì¥ì„ subtokenizeí•˜ê³ ,    \n",
        "[CLS] ì¸í’‹ë¬¸ì¥ í† í°ë“¤ [SEP] ì˜ í˜•íƒœë¡œ ë§Œë“œì„¸ìš”. \n",
        "2. í† í¬ë‚˜ì´ì¦ˆëœ ë¬¸ì¥ì„ BERTì˜ ë‹¨ì–´ì‚¬ì „ì„ ì‚¬ìš©í•´ ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•˜ì„¸ìš”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWWDkiFe4CGp"
      },
      "source": [
        "<ì‹¤í–‰ ê²°ê³¼ëŠ” ì˜ˆì‹œ>   \n",
        "- í˜•íƒœì†Œ ë¶„ì„ í›„ -> ['BERT', 'ì•Œ', 'ê³ ', 'ë³´ë‹ˆ', 'ì™„ì „', 'ì‰½ë„¤']\n",
        "- Sub-tokenizing í›„ -> ['BE', '##RT', 'ì•Œ', 'ê³ ', 'ë³´', '##ë‹ˆ', 'ì™„', '##ì „', 'ì‰½', '##ë„¤']\n",
        "- BERT ì¸í’‹ í˜•íƒœ ë³€í™˜ -> ['[CLS]', 'BE', '##RT', 'ì•Œ', 'ê³ ', 'ë³´', '##ë‹ˆ', 'ì™„', '##ì „', 'ì‰½', '##ë„¤', '[SEP]']\n",
        "- BERT ì •ìˆ˜ ì¸ë±ìŠ¤ ë³€í™˜ -> [101, 46291, 46935, 9524, 8888, 9356, 25503, 9591, 16617, 9471, 77884, 102]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "775USuqpItyW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ae57a60f-8fba-4b69-e739-b94a0a4bd46c"
      },
      "source": [
        "\"\"\" Your Code Here \"\"\"\n",
        "\n",
        "sentence = \"BERT ì•Œê³  ë³´ë‹ˆ ì™„ì „ ì‰½ë„¤\"\n",
        " \n",
        "# 1. konlpyì˜ Okt ë¶„ì„ê¸°ë¡œ ìª¼ê°œê¸°\n",
        "tokenized_sentence = new_tokenizer.tokenize(sentence)\n",
        "print(tokenized_sentence)\n",
        " \n",
        "# 2. Sub-tokenìœ¼ë¡œ ìª¼ê°œê¸°\n",
        "sub_tokenized_sent = ??? ## Your Code Here\n",
        "print(sub_tokenized_sent)\n",
        " \n",
        "# 3. [CLS] Subtokens [SEP] í˜•íƒœë¡œ ë§Œë“¤ê¸°\n",
        "sub_tokenized_sent =  ??? ## Your Code Here\n",
        "print(sub_tokenized_sent)\n",
        " \n",
        "# 4. ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•˜ê¸°\n",
        "input_ids =  ??? ## Your Code Here\n",
        "print(input_ids)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-eaec20c5b4b1>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    sub_tokenized_sent = ??? ## Your Code Here\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}