{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "실습_2_CBOW_학습하기.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/idjoopal/NLP_tensorflow2.0/blob/main/%EC%8B%A4%EC%8A%B5_2_CBOW_%ED%95%99%EC%8A%B5%ED%95%98%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EhbQCFOtd5W"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX8AhdQgtM6s"
      },
      "source": [
        "(🎧) 표시가 되어 있는 부분은 Wire 페이지에서 오디오 코드 해설을 제공하고 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IEYUqFftYIM"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQuwLrfneJGq"
      },
      "source": [
        "# 실습 2. CBOW 학습하기\n",
        "\n",
        "지난 실습에서는 위키백과 문단 코퍼스를 사용해 단어사전을 만들었습니다.   \n",
        "이번 모듈에서는 CBOW 알고리즘을 사용해, 우리 단어사전에 있는 각각의 토큰 임베딩 벡터를 학습시켜보겠습니다.   \n",
        "\n",
        "\n",
        "<b>학습 목표:    \n",
        "- CBOW 알고리즘 구조를 이해한다.\n",
        "- TensorFlow 2.0을 이용해 CBOW를 학습하고, 학습이 완료된 워드 벡터를 추출한다.</b>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTaPo6v0x3Ss"
      },
      "source": [
        "## #0. 실습 준비하기\n",
        "지난 모듈에서 저장한 단어사전과 데이터를 로드하고,   \n",
        "이번 시간에 학습한 워드 벡터를 저장하기 위해 구글 드라이브를 마운트하겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYwGgmLfJOIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1f87752-5309-4e99-d132-fa94c1d39951"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxedaxc2zfVR"
      },
      "source": [
        "#### > 라이브러리 설치\n",
        "\n",
        "Colab은 런타임에 연결될 때마다 로컬 환경이 초기화됩니다.   \n",
        "따라서 한국어 형태소 분석 패키지 konlpy를 설치하고, tokenize 함수를 정의하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wLpvrxeBP_a"
      },
      "source": [
        "konlpy에서 사용하는 jpype를 설치하고, Colab에 반영해 주어야 합니다.   \n",
        "아래 코드를 실행한 후 상단의 [런타임] - [런타임 다시 시작]을 클릭해주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSzlwmMXzgzm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99270733-b604-40cf-8757-87e00dbbff97"
      },
      "source": [
        "!pip install konlpy\n",
        "!pip install jpype1==0.7.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.3MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/96/1030895dea70855a2e1078e3fe0d6a63dcb7c212309e07dc9ee39d33af54/JPype1-1.1.2-cp36-cp36m-manylinux2010_x86_64.whl (450kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 46.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 8.0MB/s \n",
            "\u001b[?25hCollecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.11.8)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: JPype1, beautifulsoup4, tweepy, colorama, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-1.1.2 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.9.0\n",
            "Collecting jpype1==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/09/e19ce27d41d4f66d73ac5b6c6a188c51b506f56c7bfbe6c1491db2d15995/JPype1-0.7.0-cp36-cp36m-manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 7.0MB/s \n",
            "\u001b[?25hInstalling collected packages: jpype1\n",
            "  Found existing installation: JPype1 1.1.2\n",
            "    Uninstalling JPype1-1.1.2:\n",
            "      Successfully uninstalled JPype1-1.1.2\n",
            "Successfully installed jpype1-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyDRSSZU3L1_"
      },
      "source": [
        "<font color = \"red\">[MISSION]    \n",
        "적절한 형태소 분석기를 이용해    \n",
        "문장을 인풋으로 받아 형태소 분석된 리스트를 반환하는 tokenize 함수를 구현해보세요</font>  \n",
        "\n",
        "Hint: 지난 모듈에서 Komoran, Hannanum, Kkma, Okt 중 어떤 형태소 분석기를 사용했나요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_qXa9a0Ehzv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb386cd6-cb87-43c2-cf35-ece9dfb4e9b0"
      },
      "source": [
        "from konlpy.tag import Komoran, Hannanum, Kkma, Okt\n",
        "komoran = Komoran()\n",
        "\n",
        "def tokenize(sentence):\n",
        "  \"\"\" Your Code Here \"\"\"\n",
        "\n",
        "  return komoran.morphs(sentence)\n",
        "\n",
        "tokenize(\"미션 완료!!\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['미션', '완료', '!!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOwOnQRi0wOr"
      },
      "source": [
        "## #1. 학습 데이터 준비하기\n",
        "이제 CBOW 학습을 위한 데이터를 준비하겠습니다.   \n",
        "NLU 모델링을 위해서는 태스크에 적합한 인풋 데이터와 라벨을 준비해야 합니다.   \n",
        "이 과정에서 토크나이징과 인코딩이 필요합니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJb4WdIdz9qK"
      },
      "source": [
        "#### Step 1. 토크나이징\n",
        "\n",
        "\n",
        "\n",
        "<img src = \"https://github.com/seungyounglim/temporary/blob/master/fig_step1.PNG?raw=true\">\n",
        "\n",
        "먼저 학습 데이터와 검증 데이터에 있는 문단을 tokenize 함수를 이용해 토크나이징해두겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZwCKDcS1hgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7972c3bd-8dbf-4260-ce90-db3259fa433d"
      },
      "source": [
        "## 구글 드라이브에 있는 데이터셋 로딩\n",
        "import json\n",
        "with open(\"/content/gdrive/My Drive/NLP/CBOW_train_paras.json\" , 'r') as f:\n",
        "  PARAS_tr = json.loads(f.read())\n",
        "  \n",
        "with open(\"/content/gdrive/My Drive/NLP/CBOW_dev_paras.json\" , 'r') as f:\n",
        "  PARAS_dev = json.loads(f.read())\n",
        "\n",
        "print(\"Train: {} | Val: {}\".format(len(PARAS_tr), len(PARAS_dev)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 8000 | Val: 1681\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRh3o1bQ2IEl"
      },
      "source": [
        "<font color = \"red\">[MISSION]    \n",
        "PARAS_tr과 PARAS_dev에 있는 각각의 문장을 tokenize한 후,    \n",
        "각각 TOKEN_PARAS_tr와 TOKEN_PARAS_dev리스트에 append하세요</font>\n",
        "\n",
        "Hint: 실행 완료된 후 TOKEN_PARAS_tr ->\n",
        "- [ [토큰1, 토큰2, ...] , [토큰1, 토큰2, ...] ] 형태\n",
        "- 리스트의 아이템 수 len(TOKEN_PARAS_tr) = 8000\n",
        "- 각 아이템은 문장에 있는 토큰 수 만큼의 길이를 가짐"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gN9Hj4Jg1luS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af8e1465-17d2-4d73-c51f-371409411aa9"
      },
      "source": [
        "# 토크나이징된 문장을 각각 TOKEN_PARAS_tr, TOKEN_PARAS_dev에 저장\n",
        "from tqdm import tqdm\n",
        "\n",
        "TOKEN_PARAS_tr = []\n",
        "\n",
        "for para in tqdm(PARAS_tr): # PARAS_tr에 있는 각각의 자연어 문장 para에 대해 loop\n",
        "  tokenized_para = tokenize(para) # 문장을 tokenize (형태소분석)\n",
        "  TOKEN_PARAS_tr.append(tokenized_para) # 형태소분석된 문장을 TOKEN_PARAS_tr 리스트에 추가하기\n",
        "\n",
        "\n",
        "\"\"\" Your Code Here : PARAS_dev에 대해서도 같은 방식으로 문장을 토크나이즈하세요 \"\"\"\n",
        "TOKEN_PARAS_dev = []\n",
        "\n",
        "for para in tqdm(PARAS_dev): # PARAS_tr에 있는 각각의 자연어 문장 para에 대해 loop\n",
        "  tokenized_para = tokenize(para) # 문장을 tokenize (형태소분석)\n",
        "  TOKEN_PARAS_dev.append(tokenized_para) # 형태소분석된 문장을 TOKEN_PARAS_tr 리스트에 추가하기"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8000/8000 [01:31<00:00, 87.70it/s]\n",
            "100%|██████████| 1681/1681 [00:18<00:00, 90.38it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdzQGbXMFJNu"
      },
      "source": [
        "#### Step 2. 인코딩   \n",
        "토크나이징이 완료된 문단은 모델이 처리할 수 있는 의미단위인 토큰으로 나뉘었습니다.   \n",
        "그런데 아직 모델이 인식할 수 있는 정수의 형태가 아닙니다.   \n",
        "따라서 지난 시간에 만들어놓은 단어사전을 이용해 자연어 문장을 정수 인덱스로 바꾸는 기능을 코딩하겠습니다.   \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6r9LGJYksl_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c17902c-8a9a-4d42-8449-1d57c448c6f2"
      },
      "source": [
        "## 구글 드라이브에 있는 단어 리스트 로딩\n",
        "with open(\"/content/gdrive/My Drive/NLP/meta.tsv\") as f:\n",
        "  vocabulary_list = [v.strip() for v in f.readlines()]\n",
        "print(\"토큰 개수: {}개\".format(len(vocabulary_list)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "토큰 개수: 70002개\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqD-WPqPzqU3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4270e1a-dc29-4da1-e732-bfdaa3eb8075"
      },
      "source": [
        "vocabulary_list[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[PAD]', '[UNK]', '하', '이', '.', '의', '는', '을', '다', 'ㄴ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bhzyq5qPMQmO"
      },
      "source": [
        "<font color = \"red\">[MISSION] 단어사전을 이용해 토큰을 정수로 바꾸는 Python Class 만들기 </font>\n",
        "\n",
        "TextEncoder   \n",
        "  - @convert_tokens_to_ids: 토큰 리스트를 인풋으로 받아 정수 인덱스의 리스트로 리턴\n",
        "  - @convert_ids_to_tokens: 인덱스 리스트를 인풋으로 받아 원본 토큰의 리스트로 리턴\n",
        "  \n",
        "\n",
        "\n",
        "우리는 TextEncoder라는 클래스를 정의하여 단어 리스트를 인풋으로 받아    \n",
        "토큰-> 인덱스 변환과 인덱스 -> 토큰 변환을 수행하는 기능을 수행할 수 있도록 하겠습니다. \n",
        "\n",
        "\n",
        "아래 코드를 보고, 빈 칸을 채워넣어 이 두 가지 기능을 할 수 있는 TextEncoder 클래스를 완성하세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wjgxU8r_59F"
      },
      "source": [
        "class TextEncoder(object):\n",
        "    def __init__(self, vocab_list): # vocab_list를 받아서 기능을 수행함\n",
        "\n",
        "      self.pad_token = \"[PAD]\" # 딥러닝 패딩 처리를 위한 토큰 명시\n",
        "      self.oov_token = \"[UNK]\" # Unknown 토큰 처리하는 토큰 명시\n",
        "\n",
        "      ## 토큰을 인덱스로 바꾸는 dictionary 만들기 \n",
        "      token_to_id = {}\n",
        "\n",
        "      \"\"\" \n",
        "      token_to_id : vocab_list에 있는 토큰을 순서대로 토큰 - 정수로 매핑하는 dictionary\n",
        "          (예) \"[PAD]\"  : 0\n",
        "               \"[UNK]\"  : 1\n",
        "               \"하\"  : 2\n",
        "               \"이\" : 3\n",
        "               \".\" : 4\n",
        "      \"\"\"\n",
        "      ## Python에서 enumerate 함수는 리스트의 인덱스와 아이템을 튜플로 반환합니다\n",
        "      for i, token in enumerate(vocab_list):\n",
        "          token_to_id[token] = i\n",
        "\n",
        "\n",
        "      ## 인덱스를 토큰으로 바꾸는 dictionary 만들기 \n",
        "      ids_to_tokens = {v:k for k,v in token_to_id.items()}  \n",
        "\n",
        "      self.token_to_id = token_to_id\n",
        "      self.ids_to_tokens = ids_to_tokens\n",
        "      self.vocab_size = len(token_to_id)   \n",
        "      self.vocab_list = vocab_list\n",
        "    \n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "      ids = []\n",
        "\n",
        "      for token in tokens:\n",
        "        if token in self.token_to_id:\n",
        "          \"\"\"mission: 토큰을 token_to_id에서 찾을 수 있으면 해당 인덱스를 ids에 append\"\"\"\n",
        "          ids.append(self.token_to_id[token]) ## [★CODE 1★] \n",
        "          \n",
        "        else:\n",
        "          \"\"\"mission: 토큰을 token_to_id에서 찾을 수 없으면 [UNK]에 해당하는 인덱스를 append\"\"\"\n",
        "          ids.append(self.token_to_id[self.oov_token]) ## [★CODE 2★] \n",
        "          \n",
        "      return ids\n",
        "\n",
        "    def convert_ids_to_tokens(self, ids):\n",
        "      return [self.ids_to_tokens[i] for i in ids]"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih4wggX97FCF"
      },
      "source": [
        "우리 단어사전 리스트인 vocabulary_list를 인풋으로 넣어 text_encoder 클래스를 만들겠습니다.  \n",
        "이렇게 한 번 코딩을 해 놓으면, 단어 사전이 바뀌어도 같은 코드를 복사해 사용할 수 있어 편리합니다 :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjdKRykfISHQ"
      },
      "source": [
        "text_encoder = TextEncoder(vocabulary_list)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOTrLdfRC8wF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ea5804b-d310-4f49-89db-51b0e81b1b77"
      },
      "source": [
        "print(\"PAD 토큰 확인:\", text_encoder.pad_token)\n",
        "print(\"OOV 토큰 확인:\", text_encoder.oov_token)\n",
        "print(\"단어사전 크기:\", text_encoder.vocab_size)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PAD 토큰 확인: [PAD]\n",
            "OOV 토큰 확인: [UNK]\n",
            "단어사전 크기: 70002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37bu6qy67Dc4"
      },
      "source": [
        "이제 만든 함수가 정상적으로 작동하는지, 기능을 테스트해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4qwFq9bC8qk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b970a769-9def-4717-e7ca-d2bac8d0e8c0"
      },
      "source": [
        "## 단어사전을 이용해 정수 인덱스로 바꾸기\n",
        "sent = \"이 문장을 파싱해서 정수로 바꿔보겠음.\"\n",
        "tokenized_sent = tokenize(sent)\n",
        "print(tokenized_sent)\n",
        "\n",
        "text_encoder.convert_tokens_to_ids(tokenized_sent)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['이', '문장', '을', '파', '싱', '해서', '정수', '로', '바꾸', '어', '보', '겠', '음', '.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 2796, 7, 563, 5419, 5921, 13018, 24, 785, 26, 85, 192, 214, 4]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiQzFxkK7eCn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a750b4ee-351d-46d9-fdf1-b77be13f1d4a"
      },
      "source": [
        "## 정수 인덱스를 토큰으로 바꾸기\n",
        "sent = \"거꾸로 돌리기\"\n",
        "tokenized_sent = tokenize(sent)\n",
        "print(tokenized_sent)\n",
        "\n",
        "ids = text_encoder.convert_tokens_to_ids(tokenized_sent)\n",
        "print(ids)\n",
        "text_encoder.convert_ids_to_tokens(ids)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['거꾸로', '돌리', '기']\n",
            "[9706, 2048, 38]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['거꾸로', '돌리', '기']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlTRkO2n7zSE"
      },
      "source": [
        "그럼 우리가 가진 코퍼스를 방금 정의한 text_encoder를 사용해 모델이 인식할 수 있는 인덱스로 바꾸어 저장하겠습니다.   \n",
        "\n",
        "<font color = \"red\">[MISSION]    \n",
        "text_encoder를 사용해 \n",
        "- TOKEN_PARAS_tr과 TOKEN_PARAS_dev에 있는 토크나이징된 문단을 정수 인덱스로 바꾸고   \n",
        "- TOKEN_IDS_tr과 TOKEN_IDS_dev로 저장하세요 </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IfdgEZOEh3q"
      },
      "source": [
        "## Train Data\n",
        "TOKEN_IDS_tr = []\n",
        "for sent in TOKEN_PARAS_tr:\n",
        "  TOKEN_IDS_tr.append(text_encoder.convert_tokens_to_ids(sent))\n",
        "\n",
        "\n",
        "\n",
        "## Dev Data\n",
        "\"\"\" Your Code Here : 같은 방식으로 TOKEN_IDS_dev 리스트에 정수 인덱스로 바꾼 문장을 저장하세요\"\"\"\n",
        "TOKEN_IDS_dev = []\n",
        "for sent in TOKEN_PARAS_dev:\n",
        "  TOKEN_IDS_dev.append(text_encoder.convert_tokens_to_ids(sent))\n",
        "\n",
        "\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBMILe9dRwko",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbbf23f7-e100-4c11-a2e6-25865217e44d"
      },
      "source": [
        "## 코드가 정상적으로 작동했는지 확인하기\n",
        "print(\"Index Example:\", TOKEN_IDS_tr[0])\n",
        "print(text_encoder.convert_ids_to_tokens(TOKEN_IDS_tr[0]))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index Example: [1042, 2501, 9759, 22, 6, 3518, 10, 50, 22, 8, 4, 3770, 24, 1157, 17, 6, 2454, 2024, 7904, 3577, 20, 51, 71, 22, 16, 2103, 7, 19526, 2, 56, 11, 1268, 10, 4432, 449, 2103, 7, 62, 74, 1746, 10, 2749, 26, 34, 9, 3029, 12867, 5, 243, 14, 209, 26, 62, 16, 2064, 20, 1468, 37, 4, 7904, 20, 209, 26, 62, 9, 3029, 12867, 19, 7564, 29, 11940, 7, 513, 51, 97, 917, 233, 7, 41, 22, 8, 4, 1061, 34, 47, 569, 511, 724, 554, 146, 257, 7904, 6, 11941, 26, 330, 26, 39, 3632, 14, 1825, 32, 41, 22, 8, 4, 7904, 20, 338, 9, 146, 1670, 3519, 14, 515, 18, 15410, 6, 111, 3, 101, 6, 2981, 20, 2982, 30, 21, 268, 2, 37, 4, 7904, 6, 743, 15, 7, 69, 233, 7, 41, 22, 6, 5478, 3, 1208, 20, 213, 34, 47, 38, 86, 10, 17, 212, 1061, 6, 35, 3, 5479, 8, 4, 11942, 7904, 49, 2103, 7, 62, 15, 7, 69, 11, 7904, 20, 1881, 669, 36, 122, 10394, 57, 2, 56, 11, 3, 69, 4330, 74, 1918, 10, 2699, 16, 3194, 709, 6, 141, 20, 22, 56, 11, 1377, 39, 24, 1157, 17, 37, 4, 3708, 29, 11943, 13, 58, 59, 1158, 3268, 36, 3072, 3, 324, 342, 1882, 17, 26, 22, 8, 4, 22756, 11, 11944, 36, 309, 2399, 10, 11081, 9, 3633, 14, 1256, 2, 18, 511, 81, 917, 554, 2, 64, 4060, 102, 13, 580, 7, 1169, 2, 6, 35, 3, 308, 8, 4, 926, 3708, 11, 11943, 43, 7, 1228, 38, 60, 18, 1883, 10, 2455, 2, 171, 2, 6, 2663, 1047, 100, 3, 653, 6, 2345, 5, 141, 89, 9, 4761, 7, 1825, 34, 6, 124, 37, 4]\n",
            "['차량', '정비', '소가', '있', '는', '패턴', '에', '만', '있', '다', '.', '화가', '로', '추정', '되', '는', '중립', '민간인', '노숙자', 'NPC', '가', '1', '명', '있', '고', '음식', '을', '구걸', '하', '는데', ',', '종류', '에', '상관', '없이', '음식', '을', '주', '면', '지하', '에', '숨기', '어', '지', 'ㄴ', '물품', '더미', '의', '존재', '를', '알리', '어', '주', '고', '사기', '가', '올라가', 'ㄴ다', '.', '노숙자', '가', '알리', '어', '주', 'ㄴ', '물품', '더미', '에서', '보석', '과', '알콜', '을', '각각', '1', '개', '씩', '얻', '을', '수', '있', '다', '.', '돕', '지', '않', '으면', '몇', '차례', '방문', '뒤', '결국', '노숙자', '는', '굶', '어', '죽', '어', '그', '시체', '를', '가져오', 'ㄹ', '수', '있', '다', '.', '노숙자', '가', '나타나', 'ㄴ', '뒤', '누', '군가', '를', '찾', '아', '헤매', '는', '사람', '이', '오', '는', '이벤트', '가', '확률', '적', '으로', '발생', '하', 'ㄴ다', '.', '노숙자', '는', '죽이', '었', '을', '때', '얻', '을', '수', '있', '는', '아이템', '이', '가치', '가', '높', '지', '않', '기', '때문', '에', '되', '도록', '돕', '는', '것', '이', '낫', '다', '.', '간혹', '노숙자', '에게', '음식', '을', '주', '었', '을', '때', ',', '노숙자', '가', '감사', '인사', '와', '함께', '따라오', '라고', '하', '는데', ',', '이', '때', '따라가', '면', '중간', '에', '멈추', '고', '혼자', '돌아가', '는', '경우', '가', '있', '는데', ',', '버', '그', '로', '추정', '되', 'ㄴ다', '.', '식량', '과', '의약품', '은', '없', '지만', '각종', '재료', '와', '부품', '이', '매우', '많이', '소장', '되', '어', '있', '다', '.', '보리스', ',', '마르코', '와', '같이', '수집', '에', '능하', 'ㄴ', '생존자', '를', '활용', '하', '아', '몇', '번', '씩', '방문', '하', '면서', '최대한', '많', '은', '양', '을', '확보', '하', '는', '것', '이', '좋', '다', '.', '반면', '식량', ',', '의약품', '등', '을', '구하', '기', '위하', '아', '약탈', '에', '의존', '하', '아야', '하', '는', '다소', '안정', '성', '이', '떨어지', '는', '조합', '의', '경우', '크', 'ㄴ', '이득', '을', '가져오', '지', '는', '못하', 'ㄴ다', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqD7WawkRJE7"
      },
      "source": [
        "#### Step 3. CBOW 학습 가능한 형태로 변환하기\n",
        "우리가 가진 데이터는 위키피디아 문단을 토크나이징한 데이터입니다.   \n",
        "\n",
        "하지만 CBOW 학습을 위해서는 예측하고자 하는 토큰 앞뒤로 w개의 토큰을 가지고 와야 하지요.   \n",
        "예를 들어 w = 3으로 설정했다면, 각각의 train example은   \n",
        "- X : [(t-3째 토큰), (t-2째 토큰), (t-1째 토큰), (t+1째 토큰), (t+2째 토큰), (t+3째 토큰)]    \n",
        "- Y : [(t번째 토큰)]   \n",
        "의 형태여야 합니다.    \n",
        "\n",
        "아래의 generate_context_word_pairs 함수는 토크나이징된 코퍼스와 윈도우 크기를 인풋으로 받아 이러한 형태로 데이터를 만드는 역할을 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dj__4R-CG2V"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "def generate_context_word_pairs(corpus, window_size = 3):\n",
        "  \"\"\"\n",
        "  index로 바뀐 코퍼스 리스트를 인풋으로 받아\n",
        "  CBOW 학습용 input과 label을 리턴하는 함수\n",
        "  \"\"\"\n",
        "  inputs = []\n",
        "  labels = []\n",
        "\n",
        "  context_length = window_size*2\n",
        "  for words in corpus:\n",
        "    sentence_length = len(words)\n",
        "    for index, word in enumerate(words):\n",
        "      if index < window_size or index >= len(words)- window_size:\n",
        "        # window size 안에 만들 수 없는 것들은 만들지 않음.\n",
        "        continue\n",
        "\n",
        "      context_words = []            \n",
        "      start = index - window_size\n",
        "      end = index + window_size + 1\n",
        "      \n",
        "      context_words= [words[i] \n",
        "                            for i in range(start, end) \n",
        "                            if 0 <= i < sentence_length \n",
        "                            and i != index]                      \n",
        "\n",
        "      assert(len(context_words) == context_length)\n",
        "      inputs.append(context_words)\n",
        "      labels.append(word)\n",
        "\n",
        "  return inputs, labels"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quREDV9n9rkF"
      },
      "source": [
        "윈도우 크기를 3으로 설정하고, 데이터를 만들겠습니다.   \n",
        "- inputs_tr의 각 example은 길이가 6 (앞뒤로 3개, 총 6개 토큰),   \n",
        "- labels_tr의 각 example은 길이가 1 (맞춰야 하는 가운데 토큰)   \n",
        "의 형태가 됩니다.   \n",
        "\n",
        "validation 데이터도 마찬가지로 만들어줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPqBFeG3etPC"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "WINDOW_SIZE = 3\n",
        "inputs_tr, labels_tr = generate_context_word_pairs(TOKEN_IDS_tr, window_size=WINDOW_SIZE)\n",
        "inputs_dev, labels_dev = generate_context_word_pairs(TOKEN_IDS_dev, window_size=WINDOW_SIZE)\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL32QtNL-Gsa"
      },
      "source": [
        "텐서플로우에서는 numpy array 형태의 인풋을 받기 때문에 np.array 형태로 데이터를 바꾸겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLFa2wzqevJZ"
      },
      "source": [
        "inputs_tr = np.array(inputs_tr)\n",
        "labels_tr = np.array(labels_tr)\n",
        "inputs_dev = np.array(inputs_dev)\n",
        "labels_dev = np.array(labels_dev)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Mu-46F2h9hr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf7515bd-a911-4300-e070-5c9c5c3e6424"
      },
      "source": [
        "print(\"TRAIN:\", inputs_tr.shape, labels_tr.shape)\n",
        "print(\"VAL  :\", inputs_dev.shape, labels_dev.shape)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: (2013892, 6) (2013892,)\n",
            "VAL  : (420084, 6) (420084,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja-2bmhR-SbZ"
      },
      "source": [
        "👉 우리는 학습 코퍼스 8000문단을 가지고 있었는데요,    \n",
        "    각 문단을 윈도우를 주며 데이터를 만들었더니 총 2,013,892건의 많은 데이터셋이 만들어졌습니다.   \n",
        "    \n",
        "👉 이렇게 Unsupervised Learning 방식으로 쉽게 모을 수 있는 raw 코퍼스를 이용해 워드 벡터 학습에 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnPwhR7uR_y6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c679d529-c5ba-4266-cfe6-0bdda3568cbd"
      },
      "source": [
        "### CBOW 예시\n",
        "print(\"** Example**\")\n",
        "for i in range(10):\n",
        "  input_tokens = text_encoder.convert_ids_to_tokens(inputs_tr[i])\n",
        "  gt_token = text_encoder.convert_ids_to_tokens([labels_tr[i]])[0]\n",
        "  print(\"{} ___ {} -> {}\".format(\" \".join(input_tokens[:WINDOW_SIZE]),\" \".join(input_tokens[WINDOW_SIZE:]), gt_token))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "** Example**\n",
            "차량 정비 소가 ___ 는 패턴 에 -> 있\n",
            "정비 소가 있 ___ 패턴 에 만 -> 는\n",
            "소가 있 는 ___ 에 만 있 -> 패턴\n",
            "있 는 패턴 ___ 만 있 다 -> 에\n",
            "는 패턴 에 ___ 있 다 . -> 만\n",
            "패턴 에 만 ___ 다 . 화가 -> 있\n",
            "에 만 있 ___ . 화가 로 -> 다\n",
            "만 있 다 ___ 화가 로 추정 -> .\n",
            "있 다 . ___ 로 추정 되 -> 화가\n",
            "다 . 화가 ___ 추정 되 는 -> 로\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGyE471IGDs1"
      },
      "source": [
        "## #2. CBOW 학습하기\n",
        "\n",
        "<img src = \"https://github.com/seungyounglim/temporary/blob/master/fig_step3+4.PNG?raw=true\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PWmHD4S-4vB"
      },
      "source": [
        "#### Step 1. 모델 구조 만들기\n",
        "이제 데이터(인풋, 라벨)가 준비되었으니 본격적으로 모델링을 할 차례입니다.   \n",
        "keras의 Sequential 함수를 사용해 CBOW 모델 구조를 만들겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6R1f5oCkzxq"
      },
      "source": [
        "\n",
        "<font color = \"red\">[MISSION] Sequential을 이용해 CBOW 모델을 만들어보세요! </font>\n",
        "\n",
        "👉 Summary가 아래와 같아야 함\n",
        "\n",
        "```\n",
        "Model: \"sequential\"\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        "embedding (Embedding)        (None, 6, 128)            8960256   \n",
        "_________________________________________________________________\n",
        "lambda (Lambda)              (None, 128)               0         \n",
        "_________________________________________________________________\n",
        "dense (Dense)                (None, 70002)             9030258   \n",
        "=================================================================\n",
        "Total params: 17,990,514\n",
        "Trainable params: 17,990,514\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\n",
        "None\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCyst3HsS7Up",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c200dc7-424c-43e3-8c33-5adab73630bb"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Lambda\n",
        "\n",
        "VOCAB_SIZE = text_encoder.vocab_size ## 단어사전 개수\n",
        "EMBED_SIZE = 128 ## 임베딩 차원 개수\n",
        "INPUT_LENGTH = WINDOW_SIZE * 2 ## 인풋 길이\n",
        "\n",
        "\n",
        "cbow_model = Sequential()\n",
        "# 1. 임베딩 레이어 추가  ## [★CODE 1★]\n",
        "cbow_model.add(Embedding(VOCAB_SIZE, EMBED_SIZE, input_length=INPUT_LENGTH)) # 모든 임베딩 벡터는 4차원.\n",
        "\n",
        "# 2. 임베딩된 벡터들의 평균 구하기 (HINT의 Lambda 레이어 함수 사용) \n",
        "cbow_model.add(Lambda(lambda x: tf.keras.backend.mean(x, axis=1), output_shape=(EMBED_SIZE,)))\n",
        "\n",
        "# 3. 가운데 들어갈 단어를 예측하는 Fully Connected Layer 연결  ## [★CODE 2★]\n",
        "cbow_model.add(Dense(VOCAB_SIZE, activation='sigmoid'))\n",
        "\n",
        "print(cbow_model.summary())"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 6, 128)            8960256   \n",
            "_________________________________________________________________\n",
            "lambda_2 (Lambda)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 70002)             9030258   \n",
            "=================================================================\n",
            "Total params: 17,990,514\n",
            "Trainable params: 17,990,514\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR4iflDI_idv"
      },
      "source": [
        "#### Step 2. 모델 컴파일하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1sUPRoLvEeC"
      },
      "source": [
        "<font color = \"red\">[MISSION] 적당한 Loss 함수와 optimizer을 지정해 모델을 컴파일하세요!</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrYSG0_QVoiX"
      },
      "source": [
        "\"\"\" Your Code Here \"\"\"\n",
        "cbow_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z02hsjR7C3AQ"
      },
      "source": [
        "#### Step 3. 모델 학습하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pxd-MZfmA03m"
      },
      "source": [
        "<b>Callback 정의하기</b>    \n",
        "\n",
        "TensorFlow에서는 Callback을 정의해서 학습하는동안 모델을 체크할 수 있습니다.   \n",
        "\n",
        "대표적인 예시로 EarlyStoppingCallback을 사용하면 validation loss를 트래킹하다가   \n",
        " validation loss가 증가하면 학습을 알아서 멈춥니다.   \n",
        "\n",
        "여기서는 CustomCallback을 정의해서 CBOW 모델을 확인하도록 하겠습니다.   \n",
        "\n",
        "우리가 이 모델을 학습하는    \n",
        "이유는 <u>자연어 의미상 비슷한 단어를 비슷한 의미공간상에 매핑하기 위함</u>이었습니다.   \n",
        "\n",
        "학습이 진행되는동안, valid_dataset에 있는 각각의 명사 토큰에 대해 가장 가깝게 임베딩된 토큰이 어떤 것들이 있는지 확인할 것입니다.    \n",
        "이를 통해 정말 관련 있는 토큰들이 비슷한 공간에 매핑되는지 확인하겠습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6Mh3Ai3V3cj"
      },
      "source": [
        "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "  \"\"\"샘플 단어와 가장 유사한 N개 단어를 보여줌\"\"\"\n",
        "  reverse_dictionary = text_encoder.ids_to_tokens\n",
        "\n",
        "  def on_epoch_end(self, batch, logs=None):\n",
        "    valid_dataset = [\"사과\", \"취업\", \"밥\", \"사랑\", \"수학\", \"운동\", \"가수\", \"회사\", \"자신감\"]\n",
        "    valid_dataset = text_encoder.convert_tokens_to_ids(valid_dataset)\n",
        "\n",
        "    embedding = cbow_model.get_weights()[0]\n",
        "\n",
        "    reverse_dictionary = text_encoder.ids_to_tokens\n",
        "    norm = tf.keras.backend.sqrt(tf.reduce_sum(tf.keras.backend.square(embedding), 1, keepdims=True))\n",
        "    normalized_embeddings = embedding / norm\n",
        "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
        "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
        "    print(\"\")\n",
        "    for val_i in range(len(valid_dataset)):\n",
        "      valid_word = reverse_dictionary[valid_dataset[val_i]]\n",
        "      top_k = 8 # number of nearest neighbors\n",
        "      nearest = np.array(-similarity[val_i, :]).argsort()[1:top_k+1] \n",
        "      print(\"{} -> {}\".format(valid_word, \" , \".join(text_encoder.convert_ids_to_tokens(nearest))))"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IFZVUCOC9ih"
      },
      "source": [
        "model.fit() 매서드를 통해 학습을 진행하겠습니다. (🎧)     \n",
        "\n",
        "위키 백과의 일부 문단만 사용했는데도 학습이 완료되기까지 약 25분이 소요됩니다.   \n",
        "일은 GPU에게 시키고, 학습이 진행되는동안 좀 쉬다 올까요😋"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axK6is7UV3U7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3eb9492-941b-45c4-a39a-c747172eeee6"
      },
      "source": [
        "history = cbow_model.fit(inputs_tr, labels_tr, epochs = 3, batch_size=256,\n",
        "               validation_data = (inputs_dev, labels_dev),\n",
        "               callbacks = [MyCustomCallback()])\n",
        "\n",
        "# 학습 완료된 임베딩 저장하기\n",
        "final_embeddings = cbow_model.get_weights()[0]\n",
        "final_embeddings = np.array(final_embeddings)\n",
        "\n",
        "import io\n",
        "out_v = io.open(\"/content/gdrive/My Drive/NLP/vecs.tsv\" , 'w', encoding='utf-8')\n",
        "\n",
        "for num, word in enumerate( text_encoder.vocab_list):\n",
        "  vec = final_embeddings[num]\n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "out_v.close()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "7867/7867 [==============================] - ETA: 0s - loss: 6.9812\n",
            "사과 -> 정봉주 , 젊 , 악마 , 긋 , 데이비드 보위 , 유가족 , 충당 , 피고인\n",
            "취업 -> 공헌 , 태평천국 , 주체 , 수비 , 중앙집권 , 강령 , 道 , 중재자\n",
            "밥 -> 곁 , 돈 , 봉하 , 소화 , 바닥 , 침묵 , 감옥 , 찢\n",
            "사랑 -> 이별 , 헤시오도스 , 저자 , 제이크 , 가사 , 영원 , 나무 , 커버\n",
            "수학 -> 특성 , 생물학 , 규범 , 보수 , 지향 , 공유 , 전통 , 주도\n",
            "운동 -> 제도 , 개혁 , 무역 , 정책 , 기관 , 혁신 , 학술 , 법\n",
            "가수 -> 아티스트 , 아이돌 , 야구 , 최 , 각국 , 혼성 , 우수 , 집회\n",
            "회사 -> 유력 , 몽골 , 만주족 , 광해군 , 포병대 , 형제 , 사절단 , 작곡가\n",
            "자신감 -> 신령 , 선동 , 반감 , 노역 , 원동력 , 양심 , 낱말 , 환원\n",
            "7867/7867 [==============================] - 490s 62ms/step - loss: 6.9812 - val_loss: 6.1480\n",
            "Epoch 2/3\n",
            "7867/7867 [==============================] - ETA: 0s - loss: 5.9402\n",
            "사과 -> 애도 , 캐럴 , 회개 , 격려 , 주체성 , 일과 이분의 일 , 헤카 , 열망\n",
            "취업 -> 실전 , 전파천문학 , 각계각층 , 동정심 , 고국 , 軍馬 , 다방면 , 처우\n",
            "밥 -> 레아 , 지휘봉 , 신세 , 휴가 , 상자 , 목숨 , 기꺼이 , 선물\n",
            "사랑 -> 이야기 , 의상 , 스타일 , 전설 , 브루 , 정진영 , 황금충 , 조앤\n",
            "수학 -> 특성 , 신학 , 공동체주의 , 생물학 , 융합 , 의사소통 , 규범 , 작물\n",
            "운동 -> 개혁 , 정책 , 기관 , 교육 , 정당 , 혁명 , 외교 , 복지\n",
            "가수 -> 아티스트 , 솔로 , 보컬 , 아이돌 , The Boys , 정상급 , 피겨 스케이팅 , 댄스\n",
            "회사 -> 외국 , 경제 체제 , 해운 , 신성 로마 제국 , 부처 , 지배층 , 한독 , 재교육\n",
            "자신감 -> 반감 , 소질 , 배아 , 젊은이 , 즐거움 , 홀시 , 궁궐 , 장희재\n",
            "7867/7867 [==============================] - 463s 59ms/step - loss: 5.9402 - val_loss: 5.7028\n",
            "Epoch 3/3\n",
            "7867/7867 [==============================] - ETA: 0s - loss: 5.3281\n",
            "사과 -> 애도 , 회개 , 충고 , 항의 , 분노 , 변호 , 칭찬 , 질타\n",
            "취업 -> 다방면 , 게임북 , 처우 , 후삼국 , 平天下 , 한국조세연구원 , 실전 , 이성호\n",
            "밥 -> 치킨 , 목숨 , 포위망 , 손 , 꿈 , 휠체어 , ITPRO , 오반\n",
            "사랑 -> 스타일 , 전설 , 이야기 , 황금충 , 존경 , 심오 , 브루 , 극본\n",
            "수학 -> 신학 , 비평 , 천문학 , 융합 , 작물 , 공동체주의 , 문학 , 특성\n",
            "운동 -> 개혁 , 정책 , 세력 , 대책 , 혁명 , 기관 , 정당 , 교육\n",
            "가수 -> 아티스트 , 보컬 , 솔로 , 비디오 , 라이자 미넬리 , 여자 , 댄스 , 프로듀서\n",
            "회사 -> 경제 체제 , 책임자 , 문서 , 가맹 , X 밴드 , 아라비아 , 한독 , 친위대\n",
            "자신감 -> 반감 , 소질 , 지레 , 남극암치아목 , 즐거움 , 기대감 , 왕관 , 에일리어싱\n",
            "7867/7867 [==============================] - 454s 58ms/step - loss: 5.3281 - val_loss: 5.1945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Iz2MXGFiwqt"
      },
      "source": [
        "## #3. 결과 확인하기\n",
        "이제 학습 완료된 결과물을 확인하며, 비슷하게 매핑된 단어를 눈으로 확인해보겠습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InJJn_RxXg1p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "436042dc-920e-4cfb-9b0e-f678b0562ce8"
      },
      "source": [
        "## 런타임 다시 연결했을 경우 구글 드라이브 마운팅\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzUriwDzLtpM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3313788b-ec2f-4790-e0dc-f7cbb2539df6"
      },
      "source": [
        "## 저장한 단어사전과 워드벡터 로딩\n",
        "import numpy as np\n",
        "with open(\"/content/gdrive/My Drive/NLP/vecs.tsv\") as f:\n",
        "  vecs = [v.strip() for v in f.readlines()]\n",
        "final_embeddings = [v.split(\"\\t\") for v in vecs]\n",
        "final_embeddings = np.array(final_embeddings, dtype=\"float32\")\n",
        "with open(\"/content/gdrive/My Drive/NLP/meta.tsv\") as f:\n",
        "  meta = [v.strip() for v in f.readlines()]\n",
        "meta[0]"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[PAD]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r754k4xd1Dfb"
      },
      "source": [
        "CustomCallback에서 벡터상에 가장 가까이 위치한 토큰을 찾아왔건 것처럼   \n",
        "토큰을 입력하면 가장 비슷하게 매핑된 단어를 찾아오는 search_nearest 함수를 정의했습니다.   \n",
        "원하는 토큰을 넣어서 비슷한 토큰이 어떤 것이 매핑되었는지 테스트해보아요. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrcqMQhWS7JR"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "reverse_dictionary = {}\n",
        "token_to_id_dictionary = {}\n",
        "for i, token in enumerate(meta):\n",
        "  reverse_dictionary[i] = token\n",
        "  token_to_id_dictionary[token] = i\n",
        "\n",
        "def search_nearest(search_token, top_k = 5):\n",
        "  if search_token not in token_to_id_dictionary:\n",
        "    print(\"해당 토큰은 단어사전에서 찾을 수 없음\")\n",
        "  search_id = token_to_id_dictionary[search_token]\n",
        "  print(\"{} -> {}\".format(search_token, search_id))\n",
        "\n",
        "  norm = tf.keras.backend.sqrt(tf.reduce_sum(tf.keras.backend.square(final_embeddings), 1, keepdims=True))\n",
        "  normalized_embeddings = final_embeddings / norm\n",
        "  valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, [search_id])\n",
        "  similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
        "  print(\"\")\n",
        "\n",
        "  nearest = np.array(-similarity[0, :]).argsort()[1:top_k+1] \n",
        "  print(\"Nearest Tokens: {}\".format(\" , \".join([reverse_dictionary[s] for s in nearest])))"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIv_SnRgS7Cm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9e49762-1163-4173-cc43-f2fe522d4c92"
      },
      "source": [
        "search_nearest(\"학교\")"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학교 -> 368\n",
            "\n",
            "Nearest Tokens: 캠퍼스 , 이공 , 약학 , 대학교 , 서울대학교\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4mf78d2Nt4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21c52fad-856a-4b30-cd07-824414e07cbb"
      },
      "source": [
        "search_nearest(\"어머니\")"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "어머니 -> 557\n",
            "\n",
            "Nearest Tokens: 아버지 , 아내 , 딸 , 할아버지 , 아들\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j6H1HLHit6W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f14e11c-1427-4d83-f172-8a6f84832da2"
      },
      "source": [
        "search_nearest(\"이명박\")"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "이명박 -> 2370\n",
            "\n",
            "Nearest Tokens: 노무현 , 이승만 , 한나라당 , 김영삼 , 김대중\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMJhrwj5thyb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b985aee1-a412-45d2-f75d-58d45044cff0"
      },
      "source": [
        "search_nearest(\"철학\")"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "철학 -> 1241\n",
            "\n",
            "Nearest Tokens: 문학 , 논리 , 예술 , 과학 , 신학\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsIlLL6-zLZD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}