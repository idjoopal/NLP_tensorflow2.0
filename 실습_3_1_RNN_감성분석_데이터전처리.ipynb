{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "실습_3_1_RNN_감성분석_데이터전처리.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/idjoopal/NLP_tensorflow2.0/blob/main/%EC%8B%A4%EC%8A%B5_3_1_RNN_%EA%B0%90%EC%84%B1%EB%B6%84%EC%84%9D_%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%A0%84%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQuwLrfneJGq"
      },
      "source": [
        "# 실습 3. RNN을 이용한 😀감정분석😑 모델 학습하기\n",
        "\n",
        "\n",
        "\n",
        "<b>학습 목표:    \n",
        "- NLU 모델링을 위한 데이터 전처리(토크나이징 & 인코딩, 라벨 만들기)를 이해하고 Python을 사용해 코딩한다.\n",
        "</b>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI5VEKjCVzQz"
      },
      "source": [
        "## #0. 실습 준비하기\n",
        "먼저 구글 드라이브를 마운트하고, 필요한 라이브러리를 설치하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfFVQxGin7GQ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Cg9V5xCV-YJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0235e9c-7670-49b3-e184-2207e5b05901"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ695haKKAwv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40ccca5d-7f53-4815-b8cf-11c64e77a3a5"
      },
      "source": [
        "\"\"\" 한국어 형태소 분석 라이브러리 \"\"\"\n",
        "!pip install konlpy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 134kB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 7.3MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/96/1030895dea70855a2e1078e3fe0d6a63dcb7c212309e07dc9ee39d33af54/JPype1-1.1.2-cp36-cp36m-manylinux2010_x86_64.whl (450kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 50.6MB/s \n",
            "\u001b[?25hCollecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.11.8)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: beautifulsoup4, colorama, JPype1, tweepy, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-1.1.2 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5Cnajwnc73B"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7-K8wDgPMKt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "35404b48-33e6-4e4b-d10c-6a05d44bdec5"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQxrMxDHXEqF"
      },
      "source": [
        "깃허브에 있는 감성분석 데이터셋을 다운로드해 읽어오겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbVmEgziXKav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e96f4ea4-2495-4496-d574-75618f0615dc"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
        "!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-24 04:50:12--  https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14628807 (14M) [text/plain]\n",
            "Saving to: ‘ratings_train.txt’\n",
            "\n",
            "ratings_train.txt   100%[===================>]  13.95M  50.7MB/s    in 0.3s    \n",
            "\n",
            "2020-11-24 04:50:13 (50.7 MB/s) - ‘ratings_train.txt’ saved [14628807/14628807]\n",
            "\n",
            "--2020-11-24 04:50:13--  https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4893335 (4.7M) [text/plain]\n",
            "Saving to: ‘ratings_test.txt’\n",
            "\n",
            "ratings_test.txt    100%[===================>]   4.67M  24.1MB/s    in 0.2s    \n",
            "\n",
            "2020-11-24 04:50:14 (24.1 MB/s) - ‘ratings_test.txt’ saved [4893335/4893335]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7QYyS8iOnvN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dd2a73c-23d7-487c-aa73-9ee4704e9d2a"
      },
      "source": [
        "\"\"\" 데이터 읽어오기 \"\"\"\n",
        "with open(\"ratings_train.txt\") as f:\n",
        "  raw_train = f.readlines()\n",
        "with open(\"ratings_test.txt\") as f:\n",
        "  raw_test = f.readlines()\n",
        "raw_train = [t.split('\\t') for t in raw_train[1:]]\n",
        "raw_test = [t.split('\\t') for t in raw_test[1:]]\n",
        "\n",
        "FULL_TRAIN = []\n",
        "for line in raw_train:\n",
        "  if int(line[2].strip()) == 0:\n",
        "    FULL_TRAIN.append([line[0], line[1], \"부정\"])\n",
        "  elif int(line[2].strip()) == 1:\n",
        "    FULL_TRAIN.append([line[0], line[1], \"긍정\"])\n",
        "FULL_TEST = []\n",
        "for line in raw_test:\n",
        "  if int(line[2].strip()) == 0:\n",
        "    FULL_TEST.append([line[0], line[1], \"부정\"])\n",
        "  elif int(line[2].strip()) == 1:\n",
        "    FULL_TEST.append([line[0], line[1], \"긍정\"])\n",
        "\n",
        "def print_stat(name, data):\n",
        "  print(\"{:<10}: {}개 (긍정 {}, 부정 {})\".format(name, len(data), len([t for t in data if t[2]==\"긍정\"]), len(data)-len([t for t in data if t[2]==\"긍정\"])))\n",
        "\n",
        "print_stat(\"FULL_TRAIN\", FULL_TRAIN)\n",
        "print_stat(\"FULL_TEST\", FULL_TEST)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FULL_TRAIN: 150000개 (긍정 74827, 부정 75173)\n",
            "FULL_TEST : 50000개 (긍정 25173, 부정 24827)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkZC-6UY_gZl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea73fc9-2d0b-4f35-9862-a52fb2970338"
      },
      "source": [
        "# 데이터 예시 : id, 문장, 라벨 순서\n",
        "FULL_TRAIN[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['9976970', '아 더빙.. 진짜 짜증나네요 목소리', '부정'],\n",
              " ['3819312', '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나', '긍정'],\n",
              " ['10265843', '너무재밓었다그래서보는것을추천한다', '부정'],\n",
              " ['9045019', '교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정', '부정'],\n",
              " ['6483659',\n",
              "  '사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다',\n",
              "  '긍정'],\n",
              " ['5403919', '막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.', '부정'],\n",
              " ['7797314', '원작의 긴장감을 제대로 살려내지못했다.', '부정'],\n",
              " ['9443947',\n",
              "  '별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단 낫겟다 납치.감금만반복반복..이드라마는 가족도없다 연기못하는사람만모엿네',\n",
              "  '부정'],\n",
              " ['7156791', '액션이 없는데도 재미 있는 몇안되는 영화', '긍정'],\n",
              " ['5912145', '왜케 평점이 낮은건데? 꽤 볼만한데.. 헐리우드식 화려함에만 너무 길들여져 있나?', '긍정']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4ozOH9zi-pj"
      },
      "source": [
        "이제 데이터를 train / validation / test 셋으로 나누겠습니다.   \n",
        "학습 시간관계상 train 50000건, validation 10000건, test 10000건만 샘플링해서 사용하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riLyUYubXciI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3055f877-9f54-4d61-8ae8-65292c692886"
      },
      "source": [
        "import random\n",
        "\n",
        "random.seed(1)\n",
        "random.shuffle(FULL_TRAIN)\n",
        "random.shuffle(FULL_TEST)\n",
        "train = FULL_TRAIN[:50000]\n",
        "val = FULL_TRAIN[50000:60000]\n",
        "test = FULL_TEST[:10000]\n",
        "\n",
        "print_stat(\"train\", train)\n",
        "print(\".. ex:\", train[0])\n",
        "print_stat(\"validation\", val)\n",
        "print(\".. ex:\", val[0])\n",
        "print_stat(\"test\", test)\n",
        "print(\".. ex:\", test[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train     : 50000개 (긍정 24914, 부정 25086)\n",
            ".. ex: ['7570203', '장쯔이 그때나 지금이나 이뻤다', '긍정']\n",
            "validation: 10000개 (긍정 4962, 부정 5038)\n",
            ".. ex: ['8413570', '거지 발싸개같은 영화도 자막 파일이 넘쳐나는게 허다한데, 왜 이 좋은 영화는 자막을 구할수가 없을까.. 60, 70년대의 프랑스, 이탈리아, 독일의 수작들이 자막이 없어서 수OO이 지난 지금도 일반 대중에게 제대로 알려지지 못했다.', '긍정']\n",
            "test      : 10000개 (긍정 5027, 부정 4973)\n",
            ".. ex: ['1458790', '허우 샤오시엔 작품은 모두 만점!', '긍정']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLpeP2F3TI7p"
      },
      "source": [
        "## 나누어 놓은 train/ validation/ test 데이터 저장하기\n",
        "import json\n",
        "\n",
        "with open(\"/content/gdrive/My Drive/NLP/Sentiment_train.json\", \"w\") as f:\n",
        "  f.write(json.dumps(train))\n",
        "with open(\"/content/gdrive/My Drive/NLP/Sentiment_val.json\", \"w\") as f:\n",
        "  f.write(json.dumps(val))\n",
        "with open(\"/content/gdrive/My Drive/NLP/Sentiment_test.json\", \"w\") as f:\n",
        "  f.write(json.dumps(test))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHdvwAFpEcpS"
      },
      "source": [
        "# #1. 토크나이징\n",
        "\n",
        "<img src = \"https://github.com/seungyounglim/temporary/blob/master/fig_step1.PNG?raw=true\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiAtd18qhb9t"
      },
      "source": [
        "먼저 Komoran 형태소분석기를 사용해 형태소 분석 함수 tokenize를 정의하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ8uFroJgKwi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c0333bb-b326-497f-b958-c33c6003b504"
      },
      "source": [
        "from konlpy.tag import Komoran\n",
        "\n",
        "komoran = Komoran()\n",
        "\n",
        "def tokenize(sentence):\n",
        "  return komoran.morphs(sentence)\n",
        "\n",
        "tokenize(\"미션 완료!!\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['미션', '완료', '!!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK1JEphdYwyU"
      },
      "source": [
        "# #2. 단어 사전 로딩 & 인코딩\n",
        "이제 인코딩을 위해 단어사전을 로딩하겠습니다.   \n",
        "실습 소개에서 말씀드렸던 것처럼 CBOW 학습에서 사용한 70002개의 토큰에    \n",
        "이번 태스크 수행에서 새롭게 나온 토큰을 추가하겠습니다.\n",
        "<img src = \"https://github.com/seungyounglim/temporary/blob/master/fig_step2.PNG?raw=true\">   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-cDT132ciHp"
      },
      "source": [
        "#### Step 1. 단어사전 로딩하기\n",
        "- 기 학습된 단어 임베딩을 불러와 사용\n",
        "- 도메인 특화적인 단어로 [UNK]가 되는 토큰들을 식별해 단어 사전에 추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACfKwipLZEze",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a839b2c8-631b-4c9d-8b08-80f7e8924097"
      },
      "source": [
        "\"\"\" 지난 실슬 때 만들었던 단어 사전 로딩 \"\"\"\n",
        "\n",
        "## final_embeddings: 70002개 토큰에 대한 워드 벡터 매트릭스 shape=(70002, 128)\n",
        "## vocab_list: 위의 워드 벡터와 매칭되는 단어 사전\n",
        "\n",
        "with open(\"/content/gdrive/My Drive/NLP/vecs.tsv\") as f:\n",
        "  vecs = [v.strip() for v in f.readlines()]\n",
        "final_embeddings = [v.split(\"\\t\") for v in vecs]\n",
        "final_embeddings = np.array(final_embeddings, dtype=\"float32\")\n",
        "\n",
        "with open(\"/content/gdrive/My Drive/NLP/meta.tsv\") as f:\n",
        "  vocab_list = [v.strip() for v in f.readlines()]\n",
        "\n",
        "print(\"** 단어 사전 개수:\", len(vocab_list))\n",
        "print(\"** 단어벡터 예시: \")\n",
        "print(\"token :\", vocab_list[999])\n",
        "print(\"vector:\", final_embeddings[999])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "** 단어 사전 개수: 70002\n",
            "** 단어벡터 예시: \n",
            "token : 약속\n",
            "vector: [ 0.6446411   0.36585814 -0.38583678  0.5629482  -0.24461825  0.28305408\n",
            " -0.5355109  -0.34528354  0.5783807  -0.61988586 -0.2009169  -0.2548065\n",
            "  0.20425086  0.45295998 -0.12447169  0.6325752   0.47723335 -0.46177322\n",
            " -0.4572947   0.23068331 -0.00734164 -1.0122653  -0.27300954  0.00629654\n",
            "  0.30611923  0.40536442  0.29567188  0.4820554  -0.7179588   0.435078\n",
            "  0.25318748  0.55754906 -0.7791685  -0.16378227  0.21200053  0.7560081\n",
            "  0.70166504  0.2564022  -0.52721417 -0.6224764  -0.67599654 -0.18044925\n",
            "  0.6003214  -0.14806199 -0.24295808 -0.34581953  0.6212433   0.6293803\n",
            "  0.53407526  0.08164456  0.61990786 -0.1565134  -0.48517203 -0.49915248\n",
            "  0.08028579  0.7761899   0.5527567  -0.3100357   0.3895096  -0.5343472\n",
            " -0.15059426 -0.6392466  -0.05739892 -0.07412037  0.2165758  -0.38099927\n",
            " -0.6966745   0.0022627  -0.0772374   0.39921838  0.59185904  0.09193442\n",
            " -0.7117367   0.00232721  0.05522208  0.24184898  0.7188223   0.06664453\n",
            "  0.60192186 -0.16355251  0.32343397 -0.44741789 -0.00149685  0.17034498\n",
            "  0.06362036  0.95848536 -0.3167083  -0.61812407  0.7454364   0.39396593\n",
            "  0.4175227   0.25126225 -0.13284011 -0.24878569  0.29934654 -0.1199411\n",
            "  0.5558476   0.51924783  0.14647888 -0.11573921  0.45015192 -0.39242092\n",
            " -0.03654294  0.41409206 -0.88774866 -0.8739421   0.6268298   0.8440947\n",
            "  0.6098481  -0.44067284 -0.11671774  0.40805656 -0.0026393   0.01279358\n",
            " -0.29379305 -0.0834411   0.36521795 -0.06466522  0.10955771 -0.40862578\n",
            "  0.40438366 -0.8537584   0.5366915  -0.07844007  0.29338717 -0.31353107\n",
            "  0.5732439   0.35582712]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdHzLQxYSlWI"
      },
      "source": [
        "<font color=\"blue\"> 🙋‍♀️[QUIZ] final_embeddings의 차원이 70002 x 128인 이유는 무엇인가요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJDxTsjChDKs"
      },
      "source": [
        "현재 태스크에서 [UNK]로 떨어지는 단어가 어떤 것들이 있는지 살펴보겠습니다.   \n",
        "CBOW 실습에서 사용했던 collections의 Counter()기능을 사용하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IisJ8WUwZUV7"
      },
      "source": [
        "import collections \n",
        "from tqdm import tqdm\n",
        "tot_tokens = 0\n",
        "oov_counter = collections.Counter() #새로운 토큰 카운터\n",
        "\n",
        "Tokenized_train = [] #토크나이징된 데이터 저장\n",
        "for dat in tqdm(train):\n",
        "  sent = dat[1]\n",
        "  tokenized_sent = tokenize(sent)\n",
        "  tot_tokens += len(tokenized_sent) #토큰 개수\n",
        "  for word in tokenized_sent:\n",
        "    if word not in vocab_list: #기존 단어사전에서 찾을 수 없으면\n",
        "      oov_counter[word] += 1 #Counter에서 개수 세기\n",
        "      \n",
        "  Tokenized_train.append([dat[0],tokenized_sent, dat[2]]) # 토크나이징된 문장 저장"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8WfRvinZUh8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feb69b20-7b9a-4d4e-e22c-8b12b12281cf"
      },
      "source": [
        "print(\"# OOV Tokens:\", len(oov_counter))\n",
        "print(\"{}/{} ({:.2f}%) are [UNK] in train tokens\".format(sum(oov_counter.values()) , tot_tokens , 100*sum(oov_counter.values())/tot_tokens))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# OOV Tokens: 15927\n",
            "36292/955855 (3.80%) are [UNK] in train tokens\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwbm16L8T3kz"
      },
      "source": [
        "👉 50,000개의 train 문장에서 무려 15,000개 이상의 Out-Of-Vocabulary 토큰들이 발견되었습니다.    \n",
        "🙋‍♀️ 어떤 토큰들이 기존 단어 사전에 포함되어 있지 않았을까요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9O8yqJea7E6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8af04c8f-3e2d-4a83-cf7b-14b1aca8e6ab"
      },
      "source": [
        "most_common = oov_counter.most_common(len(oov_counter))\n",
        "print(most_common[:10])\n",
        "print(most_common[-10:])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('ㅋㅋ', 647), ('......', 601), ('♥', 472), ('ㅋㅋㅋ', 358), ('ㅋ', 358), ('ㅠㅠ', 321), ('이쁘', 314), ('ㅎㅎ', 297), ('막장', 260), ('ㅡㅡ', 234)]\n",
            "[('스토리임....ㅡㅡ', 1), ('어렷을떄', 1), ('섬세하뮤ㅠ', 1), ('없슴다', 1), ('hhhh', 1), ('린날', 1), ('너거', 1), ('재밋었을듯..', 1), ('장음', 1), ('죽다ㅠ_ㅠ', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbXz1rwmUMHH"
      },
      "source": [
        "예상했던 것과 같이 구어체적인 표현들이 새로 등장한 것을 볼 수 있습니다.   \n",
        "이제 새로 나온 토큰들을 기존의 단어사전에 추가해 감성분석 태스크 분석을 위한 새로운 단어사전을 만들겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSdRpNRLafFu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3a38e9b-292d-4f2f-a10d-126797dfb2a8"
      },
      "source": [
        "### Train 데이터에서 새로 발견한 토큰을 기존 단어 사전에 추가\n",
        "new_vocab_list = vocab_list.copy() # 1. 기존 단어 사전 복사\n",
        "new_vocab_list.extend([v[0] for v in most_common]) # 2. 새로 나온 토큰을 기존 리스트에 이어붙이기\n",
        "print(\"# New Vocabs = {}\".format(len(new_vocab_list)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# New Vocabs = 85929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0m667zbUon4"
      },
      "source": [
        "<font color=\"blue\"> 🙋‍♀️[QUIZ] 새로 나온 토큰을 기존 리스트의 뒤에 이어붙여야 하는 이유는 무엇인가요?   \n",
        "🙋‍♀️[QUIZ] 새로운 단어사전은 몇 개의 토큰이 있나요? 모델에서 임베딩 차원은 몇 차원이 되어야 하나요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbgukIuZU7Lk"
      },
      "source": [
        "새로 만든 단어사전을 저장해 놓아야 이후 학습~추론에서 사용할 수 있겠지요? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piDQCOt_0i5Z"
      },
      "source": [
        "## 새로 만든 단어사전 저장\n",
        "with open(\"/content/gdrive/My Drive/NLP/Sentiment_vocab.json\", \"w\") as f:\n",
        "  f.write(json.dumps(new_vocab_list))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZZ2J9kUWqq_"
      },
      "source": [
        "#### Step 2. 데이터 토크나이징하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fGQbvXbVLcM"
      },
      "source": [
        "그럼 validation과 test 데이터에 대해서도 토크나이징을 진행하겠습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWG3pA0p2e0L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9b12029-acff-4931-a91c-7dbe7aef8b04"
      },
      "source": [
        "Tokenized_val = [] #토크나이징된 데이터 저장\n",
        "for dat in tqdm(val):\n",
        "  tokenized_sent = tokenize(dat[1])   \n",
        "  Tokenized_val.append([dat[0],tokenized_sent, dat[2]]) # 토크나이징된 문장 저장\n",
        "\n",
        "Tokenized_test = [] #토크나이징된 데이터 저장\n",
        "for dat in tqdm(test):\n",
        "  tokenized_sent = tokenize(dat[1])   \n",
        "  Tokenized_test.append([dat[0],tokenized_sent, dat[2]]) # 토크나이징된 문장 저장"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:06<00:00, 1470.06it/s]\n",
            "100%|██████████| 10000/10000 [00:06<00:00, 1446.14it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avaO54uS2e5j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "847cf025-6864-4a53-ad23-9089c27cfdfc"
      },
      "source": [
        "print(len(Tokenized_train) , len(Tokenized_val), len(Tokenized_test))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000 10000 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enZmxc4OWhTC"
      },
      "source": [
        "#### Step 3. 텍스트 인코더 코딩하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnuO8hmMWHIR"
      },
      "source": [
        "토크나이징된 문장을 인덱스로 바꾸기 위해, 지난 모듈에서 정의했던 TextEncoder를 사용하겠습니다.    \n",
        "지난 시간에 코딩했던 내용을 utils.py에 담아두었습니다.   \n",
        "여기서 TextEncoder를 불러와 사용하도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaZkpgi9Eh5a"
      },
      "source": [
        "from utils import TextEncoder\n",
        "text_encoder = TextEncoder(new_vocab_list)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1odlXGscJ1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb03f30-e59b-49fb-bbd5-a9cba4d280f1"
      },
      "source": [
        "## 토크나이저 테스트\n",
        "sent = \"새로 만든 단어사전 ♥\"\n",
        "tokenized_sent = tokenize(sent)\n",
        "tokenized_ids = text_encoder.convert_tokens_to_ids(tokenized_sent) # 토큰 -> 인덱스\n",
        "reversed_token= text_encoder.convert_ids_to_tokens(tokenized_ids) # 인덱스 -> 토큰\n",
        "\n",
        "print(tokenized_ids)\n",
        "print(reversed_token)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1339, 128, 9, 1313, 1815, 70004]\n",
            "['새로', '만들', 'ㄴ', '단어', '사전', '♥']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_8dBaSmcqCY"
      },
      "source": [
        "### Step 4. 태스크 수행을 위한 형태로 만들기   \n",
        "그럼 이제 본격적으로 감성분석 태스크를 수행할 수 있는 형태로 인풋을 준비하겠습니다.   \n",
        "\n",
        "이를 위해 아래의 세 가지를 수행해야 합니다.   \n",
        "1. 인풋 자연어 토큰을 정수 인덱스로 변환(@text_encoder.convert_tokens_to_ids)\n",
        "2. 정답 라벨을 정수 인덱스로 변환 -> 라벨 매핑 사전 필요   \n",
        "3. 배치 처리를 위해 패딩 & numpy array로 변환\n",
        "\n",
        "\n",
        "위의 세 가지 작업을 수행하는 함수로 create_cls_feature이라는 함수를 정의하겠습니다.    \n",
        "TextEncoder와 마찬가지로, 이 함수는 다양한 텍스트 분류 과제에서 코드 재활용이 가능합니다. \n",
        "\n",
        "create_cls_feature\n",
        "- 함수 인풋: 데이터 , text_encoder, max_seq_len\n",
        "- 함수 아웃풋: input_ids , labels, label_map   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5UdYcieZ7ef"
      },
      "source": [
        "그럼 데이터 example 일부에 대해 한 단계씩 차례대로 수행하며 차례대로 코딩해보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWGM-lpgYblR"
      },
      "source": [
        "<b>1. 인풋 자연어 토큰을 정수 인덱스로 변환(@text_encoder.convert_tokens_to_ids)   </b>   \n",
        "함수는 인풋으로 Tokenized_train과 같은 데이터를 받습니다.   \n",
        "데이터는 리스트의 리스트로 이루어져 있으며, 각각의 리스트는 [문장 번호, 토큰화된 문장, 정답 라벨]의 모양으로 되어 있습니다.   \n",
        "(예) ['7570203', ['장쯔이', '그때', '나', '지금', '이나', '이뻤다'], '긍정']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMFeFXFoYwlI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d97fe420-d736-47eb-b5d2-1d74a188e5a2"
      },
      "source": [
        "examples = Tokenized_train[:3]\n",
        "\n",
        "for example in examples:\n",
        "  print(example)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['7570203', ['장쯔이', '그때', '나', '지금', '이나', '이뻤다'], '긍정']\n",
            "['6317334', ['음식', '가지', '고', '사람', '을', '해치', 'ㄴ', '사람', '이', '명장', '되', '고', ',', '인', '주', '는', '끝', '까지', '반성', '도', '없', '구', ',', '막장', '드라마', '로', '기억', '되', 'ㄹ', '듯'], '부정']\n",
            "['6287811', ['뭔내용인지도모르겟고', '돈', '아깝', '고'], '부정']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjWcIQxqZPbo"
      },
      "source": [
        "<font color=\"red\"> [MISSION] examples에 있는 토큰화된 자연어 문장들을 인덱스로 변환해 input_ids라는 리스트에 저장해보세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfS-sNNIZO_H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fadb3b8-7fff-44d4-ad1b-68da16f4c9aa"
      },
      "source": [
        "input_ids = []\n",
        "\n",
        "for example in examples:\n",
        "  idx, tokenized_sent, label = example\n",
        "  input_id = text_encoder.convert_tokens_to_ids(tokenized_sent)\n",
        "  input_ids.append(input_id)\n",
        "\n",
        "for i, input_id in enumerate(input_ids):\n",
        "  print(\"문장 {}:\".format(i), input_id)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "문장 0: [34556, 1862, 75, 467, 150, 71299]\n",
            "문장 1: [2103, 110, 16, 111, 7, 12533, 9, 111, 3, 11094, 17, 16, 11, 435, 62, 6, 540, 61, 5730, 31, 58, 562, 11, 70010, 526, 24, 1478, 17, 32, 1036]\n",
            "문장 2: [73219, 762, 11267, 16]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4igxqa1aFT8"
      },
      "source": [
        "<b>2. 정답 라벨을 정수 인덱스로 변환 -> 라벨 매핑 사전 필요    </b>   \n",
        "\n",
        "다음으로 모델에게 라벨을 알려줄 수 있는 라벨 매핑 사전이 필요합니다.    \n",
        "모델은 \"긍정\"이나 \"부정\" 이라는 단어를 인식할 수 있기 때문에, 각 라벨에 해당하는 정수 인덱스를 매핑해주는 것입니다.   \n",
        "{\"긍정\": 0 , \"부정\": 1} 이런 식으로요.   \n",
        "\n",
        "이 때 라벨 매핑 사전은 학습~추론에서 유지되어야 하겠지요?   \n",
        "따라서 학습 데이터를 이용해   \n",
        "1. 새로 발견된 라벨을 라벨 매핑 사전에 저장한다.\n",
        "2. 완성된 라벨 매핑 사전을 저장해둔다.   \n",
        "\n",
        "이후 검증/ 테스트 데이터에서는 위에서 만든 라벨 매핑 사전을 사용해 매핑만 해주도록 코드를 구현하겠습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMwuDNZlb673",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03b3347f-a6c3-4157-d9ec-4e89d30b3d8d"
      },
      "source": [
        "CREATE_LABEL_MAP = True # 새로 사전을 만들겠다. \n",
        "label_map = {}\n",
        "label_index = 0 \n",
        "\n",
        "label_ids = [] ## 데이터에 대해 변환된 라벨 저장\n",
        "for example in examples:\n",
        "  idx, tokenized_sent, label = example\n",
        "  if label not in label_map: # 라벨이 label_map에 없다면\n",
        "    if CREATE_LABEL_MAP:\n",
        "      label_map[label] = label_index # 라벨 맵에 해당 라벨 추가\n",
        "      label_index += 1\n",
        "      label_id = label_map[label]      \n",
        "    else:\n",
        "      ## train 이외의 단계에서 새로운 라벨이 발견되었다면, 이는 잘못된 데이터입니다. \n",
        "      ## 따라서 Error 메시지를 추가하고, 건너뛰도록 하겠습니다.\n",
        "      print(\"** ERROR: UNSEEN LABEL DETECTED -\", label)\n",
        "      continue\n",
        "  else: # 라벨을 찾았으면\n",
        "    label_id = label_map[label]\n",
        "  label_ids.append(label_id)\n",
        "\n",
        "print(\"라벨 매핑 사전:\", label_map)\n",
        "print(\"-> 원래 라벨:\", [e[2] for e in examples])\n",
        "print(\"-> 인덱싱 후:\", label_ids)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "라벨 매핑 사전: {'긍정': 0, '부정': 1}\n",
            "-> 원래 라벨: ['긍정', '부정', '부정']\n",
            "-> 인덱싱 후: [0, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7-rRYFKda7k"
      },
      "source": [
        "<b>3. 배치 처리를 위해 패딩 & numpy array로 변환    </b>   \n",
        "\n",
        "정수 인덱스로 변환한 input_ids를 살펴볼까요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B27syNaIdpA1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0e32727-8166-484e-9b4d-33fba5f15526"
      },
      "source": [
        "for i, input_id in enumerate(input_ids):\n",
        "  print(\"문장 {}:\".format(i), input_id)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "문장 0: [34556, 1862, 75, 467, 150, 71299]\n",
            "문장 1: [2103, 110, 16, 111, 7, 12533, 9, 111, 3, 11094, 17, 16, 11, 435, 62, 6, 540, 61, 5730, 31, 58, 562, 11, 70010, 526, 24, 1478, 17, 32, 1036]\n",
            "문장 2: [73219, 762, 11267, 16]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkE_YPI_dqMG"
      },
      "source": [
        "👉자연어 문장은 길이가 제각각이기 때문에 각 문장의 길이가 제각각인 것을 볼 수 있습니다.   \n",
        "\n",
        "하지만 딥러닝 배치 처리를 위해서는 이들을 일정한 길이로 맞춰주어야 하지요.   \n",
        "이 기능을 수행해 주는 것이 tensorflow.keras.preprocessing.sequence의 pad_sequences 기능입니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bARuY8gfYkDs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cc31b6f-d7c0-43b0-cbf9-739348da9ac1"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "max_seq_len = 20\n",
        "\n",
        "input_ids_1 = pad_sequences(input_ids, \n",
        "                            maxlen=max_seq_len, # 최대 문장 길이\n",
        "                            padding=\"post\", # 패딩은 문장의 뒤에 한다\n",
        "                            truncating=\"post\" ) # 최대 길이를 넘어갈 경우 뒷 부분을 자른다\n",
        "print(\"** 예시 1:\")\n",
        "print(input_ids_1)\n",
        "input_ids_2 = pad_sequences(input_ids, \n",
        "                            maxlen=max_seq_len, # 최대 문장 길이\n",
        "                            padding=\"post\", # 패딩은 문장의 앞에 한다\n",
        "                            truncating=\"pre\" ) # # 최대 길이를 넘어갈 경우 앞 부분을 자른다\n",
        "print(\"\\n** 예시 2:\")\n",
        "print(input_ids_2)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "** 예시 1:\n",
            "[[34556  1862    75   467   150 71299     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [ 2103   110    16   111     7 12533     9   111     3 11094    17    16\n",
            "     11   435    62     6   540    61  5730    31]\n",
            " [73219   762 11267    16     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]]\n",
            "\n",
            "** 예시 2:\n",
            "[[34556  1862    75   467   150 71299     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [   17    16    11   435    62     6   540    61  5730    31    58   562\n",
            "     11 70010   526    24  1478    17    32  1036]\n",
            " [73219   762 11267    16     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUeAbTSPYjdQ"
      },
      "source": [
        "<font color=\"red\"> [MISSION] 한국어에서 핵심적인 정보는 문장의 뒷 부분에 있다고 판단하였습니다.   \n",
        "따라서, 문장이 max_seq_len을 넘어가면 앞부분을 자르고, 모자르면 문장 뒤에 0 패딩을 하기로 결정하였습니다.   \n",
        "pad_sequences 함수를 사용해 이 기능을 수행할 수 있도록 직접 코딩해보세요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzMIs9ehfAKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e2039cb-9e25-43d5-81a5-3ab8039ad6f5"
      },
      "source": [
        "\"\"\" Your Code Here \"\"\"\n",
        "input_ids = pad_sequences(input_ids,\n",
        "                            maxlen=max_seq_len,\n",
        "                            padding=\"post\",\n",
        "                            truncating=\"pre\")\n",
        " \n",
        "print(input_ids)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[34556  1862    75   467   150 71299     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [   17    16    11   435    62     6   540    61  5730    31    58   562\n",
            "     11 70010   526    24  1478    17    32  1036]\n",
            " [73219   762 11267    16     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uzYr10RfIYV"
      },
      "source": [
        "그럼 위에서 하나씩 만들었던 함수를 모두 합쳐서 create_cls_feature 함수를 만들겠습니다.   \n",
        "이 함수는 utils.py에도 저장되어 있어, 향후에도 텍스트 분석에서 활용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Upowc1lDca9w"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "def create_cls_feature(examples, text_encoder, max_seq_len, label_map=None):\n",
        "\n",
        "  input_ids = [] # 정수 인덱스로 변환한 문장들의 리스트\n",
        "  labels = [] # 정답 라벨 리스트\n",
        "\n",
        "  if label_map is None: #label_map이 없으면 -> 이번에 데이터를 처리하면서 새로 생성함\n",
        "    CREATE_LABEL_MAP = True\n",
        "    label_map = {}\n",
        "    label_index = 0\n",
        "  else: \n",
        "    CREATE_LABEL_MAP = False\n",
        "    print(\"** Start creating features using label map\")\n",
        "    print(label_map)\n",
        "\n",
        "  for example in examples:\n",
        "    idx, tokenized_sent, label = example\n",
        "\n",
        "    ## 1. text_encoder 사용해 정수로 변환 \n",
        "    input_id = text_encoder.convert_tokens_to_ids(tokenized_sent)\n",
        "    if len(input_id) == 0:\n",
        "      print(\"Sentence with length = 0... continue\", example)\n",
        "      continue\n",
        "\n",
        "    ## 2. label 매핑 & index로 변환\n",
        "    if label in label_map:\n",
        "      label_id = label_map[label]\n",
        "    else:\n",
        "      if CREATE_LABEL_MAP:\n",
        "        # label map에 추가\n",
        "        label_map[label] = label_index\n",
        "        label_index += 1\n",
        "        label_id = label_map[label]\n",
        "      else:\n",
        "        print(\"** ERROR: UNSEEN LABEL DETECTED -\", label)\n",
        "        continue\n",
        "  \n",
        "    ## 전체 리스트에 append\n",
        "    input_ids.append(input_id)\n",
        "    labels.append(label_id)\n",
        "    \n",
        "      \n",
        "  \"\"\" max_seq_len을 넘는 문장은 절단, 모자르는 것은 PADDING \"\"\"\n",
        "  input_ids = pad_sequences(input_ids, \n",
        "                            maxlen=max_seq_len, \n",
        "                            padding=\"post\", \n",
        "                            truncating=\"pre\")\n",
        "\n",
        "  ## np.array로 변환해 리턴\n",
        "  input_ids = np.array(input_ids)\n",
        "  labels = np.array(labels)\n",
        "  \n",
        "  assert len(input_ids) == len(labels)\n",
        "  print(\"** {} examples processed\".format(len(input_ids)))\n",
        "\n",
        "  return input_ids, labels, label_map\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AchPbrDWl4X_"
      },
      "source": [
        "함수를 사용해 train, validation, test 데이터를 각각 처리하겠습니다.   \n",
        "train 데이터를 만들 때는 label_map을 인풋에 주지 않아, 전처리중 label_map이 생성되도록 합니다.   \n",
        "validation과 test 데이터를 만들 때에는 학습 데이터에 대해 만들어진 label_map을 사용해 전처리를 진행합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUuJxeHtnTBU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adf41ea4-2c7a-4514-d43d-62368c6a04fd"
      },
      "source": [
        "# TRAIN\n",
        "train_ids, train_labels, label_map = create_cls_feature(Tokenized_train, text_encoder, max_seq_len=50, label_map = None)\n",
        "# VAL\n",
        "val_ids, val_labels, _ = create_cls_feature(Tokenized_val, text_encoder, max_seq_len=50, label_map = label_map)\n",
        "# TEST\n",
        "test_ids, test_labels, _ = create_cls_feature(Tokenized_test, text_encoder, max_seq_len=50, label_map = label_map)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence with length = 0... continue ['5942978', [], '부정']\n",
            "** 49999 examples processed\n",
            "** Start creating features using label map\n",
            "{'긍정': 0, '부정': 1}\n",
            "Sentence with length = 0... continue ['2172111', [], '긍정']\n",
            "** 9999 examples processed\n",
            "** Start creating features using label map\n",
            "{'긍정': 0, '부정': 1}\n",
            "** 10000 examples processed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FRAzerdca6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f8895fe-007c-41ab-fa05-596258e878c7"
      },
      "source": [
        "print(\"# Train={} # Val={} # Test={}\".format(len(train_ids), len(val_ids), len(test_ids)))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Train=49999 # Val=9999 # Test=10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cep29zj6hLjV"
      },
      "source": [
        "모델링을 위한 데이터 전처리가 완료되었습니다!    \n",
        "그럼 데이터를 저장하고, 다음 페이지에서 모델링을 계속 진행하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYZdu4fphWcS"
      },
      "source": [
        "import pickle\n",
        "prepro_data = {\n",
        "    \"train_ids\": train_ids,\n",
        "    \"train_labels\": train_labels,\n",
        "    \"val_ids\": val_ids,\n",
        "    \"val_labels\": val_labels,\n",
        "    \"test_ids\": test_ids,\n",
        "    \"test_labels\": test_labels,\n",
        "    \"label_map\":label_map\n",
        "}\n",
        "\n",
        "with open(\"/content/gdrive/My Drive/NLP/Sentiment_prepro_data.pkl\", \"wb\") as f:\n",
        "  pickle.dump(prepro_data, f)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxUy68PJiIze"
      },
      "source": [
        "utils.py 파일은 드라이브에 저장해, 향후 활용할 수 있도록 하겠습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO_pv12H9YQZ"
      },
      "source": [
        "!cp utils.py \"/content/gdrive/My Drive/NLP/\""
      ],
      "execution_count": 32,
      "outputs": []
    }
  ]
}